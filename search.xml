<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pytorch学习之优化器（optim）]]></title>
    <url>%2F2018%2F12%2F24%2FPytorch%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88optim%EF%BC%89%2F</url>
    <content type="text"><![CDATA[PyTorch基础，主要解释torch.optim中的函数方法|优化方法 [TOC] 构建 为了构建一个Optimizer，你需要给它一个包含了需要优化的参数（必须都是Variable对象）的iterable。然后，你可以设置optimizer的参 数选项，比如学习率，权重衰减，等等。 12optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)optimizer = optim.Adam([var1, var2], lr = 0.0001) 为特定层单独设置参数123optim.SGD([&#123;'params':model.base.parameters()&#125;, &#123;'params':model.base2.parameters(), 'lr':1e-3&#125;] ,lr=1e-2,momentum=0.9) 执行优化器123456for input, target in dataset: optimizer.zero_grad() output = model(input) loss=loss_fn(output, target) loss.backward() optimizer.step() 载入与保存123state_dict()load_state_dict(state_dict) 常用优化算法 123torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9,0.999), weight_decay=1e-5)optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) 改变学习率手动改变学习率12for param in optimizer.param_groups: param['lr'] = 1e-4 自定义变学习率12345678torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)&gt;&gt;&gt; # Assuming optimizer has two groups.&gt;&gt;&gt; lambda1 = lambda epoch: epoch // 30&gt;&gt;&gt; lambda2 = lambda epoch: 0.95 ** epoch&gt;&gt;&gt; scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])&gt;&gt;&gt; for epoch in range(100):&gt;&gt;&gt; scheduler.step() 固定步长变学习率12345678910torch.optim.lr_scheduler.StepLR(optimizer, step_size,gamma=0.1,last_epoch=-1)&gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups&gt;&gt;&gt; # lr = 0.05 if epoch &lt; 30&gt;&gt;&gt; # lr = 0.005 if 30 &lt;= epoch &lt; 60&gt;&gt;&gt; # lr = 0.0005 if 60 &lt;= epoch &lt; 90&gt;&gt;&gt; # ...&gt;&gt;&gt; scheduler = StepLR(optimizer, step_size=30, gamma=0.1)&gt;&gt;&gt; for epoch in range(100):&gt;&gt;&gt; scheduler.step() 自适应变学习率12345678910#当精度不再提高时，变学习率torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, cooldown=10, min_lr=0, eps=1e-08)modemode='min'/'max' factor=0.1 学习率变化因子patience=10 如果10次没有检测到精度提高，则改变学习率verbose=False #改变学习率同时打印threshold=0.0001 #比较精度cooldown=10 #改变学习率后10次内不再变化]]></content>
      <categories>
        <category>python</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
        <tag>优化方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCN读书笔记]]></title>
    <url>%2F2018%2F12%2F18%2FFCN%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[经典语义分割网络FCN，读书笔记，文献翻译 参考译文：用于语义分割的全卷积网络FCN（UC Berkeley [TOC] 摘要 卷积网络在特征分层领域是非常强大的视觉模型。我们证明了经过端到端、像素到像素训练的卷积网络超过目前语义分割中最先进的技术。我们的核心观点是建立 “全卷积” 网络，输入任意尺寸，经过有效的推理和学习产生相应尺寸的输出。 我们改编当前的分类网络(AlexNet ,the VGG net , and GoogLeNet ) 到完全卷积网络。然后我们定义了一个跳跃式的架构，结合来自深、粗层的语义信息和来自浅、细层的表征信息来产生准确和精细的分割。 1 介绍 Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a localto-global pyramid. We define a novel “skip” architecture to combine deep, coarse, semantic information and shallow, fine。 语义分割面临在语义和位置的内在张力问题：全局信息解决的 “是什么”，而局部信息解决的是 “在哪里”。深层特征通过非线性的局部到全局金字塔编码了位置和语义信息。我们定义了一种利用集合了深、粗层的语义信息和浅、细层的表征信息的特征谱的跨层架构。 2 相关工作 Dense prediction with convnets Several recent works have applied convnets to dense prediction problems…Common elements of these approaches include • small models restricting capacity and receptive fields• patchwise training• post-processing by superpixel projection, random fieldregularization, filtering, or local classification• input shifting and output interlacing for dense output as introduced by OverFeat• multi-scale pyramid processing• saturating tanh nonlinearities and• ensembles 这些方法的相同点包括如下： 限制容量和接收域的小模型 patchwise 训练 超像素投影的预处理，随机场正则化、滤波或局部分类 输入移位和 dense 输出的隔行交错输出 多尺度金字塔处理 饱和双曲线正切非线性 集成 whereas our method does without this machinery. However, we do study patchwise training and “shift-and-stitch” dense output from the perspective of FCNs. We also discuss in-network sampling, of which the fully connected prediction by Eigen et al. is a special case. 然而我们的方法确实没有这种机制。但是我们研究了 patchwise 训练 和从 FCNs 的角度出发的 “shift-and-stitch”dense 输出。我们也讨论了网内上采样，其中 Eigen 等人[7] 的全连接预测是一个特例。 3 全卷积网络 Each layer of data in a convnet is a three-dimensional array of size h × w × d, where h and w are spatial dimensions, and d is the feature or channel dimension. The first layer is the image, with pixel size h × w, and d color channels. Locations in higher layers correspond to the locations in the image they are path-connected to, which are called their receptive fields. 卷积网的每层数据是一个h × w × d 的三维数组，其中 h 和 w 是空间维度, d 是特征或通道维数。第一层是像素尺寸为 h × w、颜色通道数为 d 的图像。高层中的 locations 和图像中它们连通的 locations 相对应，被称为接收域。 3.1 网络改编去掉网络中的全连接层，用卷积层代替。 3.2 Shift-and stitch 是滤波稀疏3.3 上采样是向后向卷积 Another way to connect coarse outputs to dense pixels is interpolation. 使用双线性插值可以从粗糙输出到密集像素输出。 Note that the deconvolution filter in such a layer need not be fixed (e.g., to bilinear upsampling), but can be learned. A stack of deconvolution layers and activation functions can even learn a nonlinear upsampling. 需要注意的是反卷积滤波在这一层不需要被固定不变（比如双线性上采样）但是可以被学习。一堆反卷积层和激励函数甚至能学习一种非线性上采样。 3.4 patchwise 训练是一种损失采样 Sampling in patchwise training can correct class imbalance and mitigate the spatial correlation of dense patches . In fully convolutional training, class balance can also be achieved by weighting the loss, and loss sampling can be used to address spatial correlation. patchwise可以实现纠正类的不平衡性和缓和像素密集图的空间相关性。 前者全卷积网络可以通过改变loss的权重实现，后者通过patchwise实现。 4 分割框架4.1 从分类到 dense FCN使用VGG16效果比较好，用固定的学习率效果更好，通过额外的数据可以提高mIU。 4.2 结合 “是什么” 和“在哪里” 4.3 实验优化参数 使用SGD，momentum =0.9 FCN-AlexNet, FCN-VGG16, and FCN-GoogLeNet 学习率分别为10^(-3), 10^(-4), 和 5^(-5) L2正则化 weight= 5^(-4) 或者 2^(-4) 微调全局微调效果要好于局部微调 分类平衡全卷积训练通过权重对损失函数进行类平衡 测量方法度量 我们从常见的语义分割和场景解析评估中提出四种度量，它们在像素准确率和在联合的区域交叉上是不同的。令 n_ij 为类别 i 的被预测为类别 j 的像素数量，有 n_ij 个不同的类别，令 为类别 i 的像素总的数量。我们将计算： 附录 A IU 上界 我们通过下采样 ground truth 图像，然后再次对它们进行上采样，来模拟可以获得最好的结果，其伴随着特定的下采样因子。 factor mIU 128 50.9 64 73.3 32 86.1 16 92.8 8 96.4 4 98.5]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
        <tag>文献</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SegNet读书笔记]]></title>
    <url>%2F2018%2F12%2F14%2FSegNet%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[经典语义分割网络SegNet，读书笔记，文献翻译 摘要 We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. 逐个像素语义分割 This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. SegNet核心是编码网络+像素级分类层+解码网络 The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. 编码网络取了VGG16的前13层（附带BN层）。解码网络就是将低分辨率的特征图解码为输入图片的分辨率图。 The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. SegNet网络核心是对低分辨的特征图进行上采样解码。在编码时使用了池化索引来帮助解码时的非线性上采样。这是上采样的特征图是稀疏的，用可训练的滤波器来产生密集的特征图。 介绍 In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images. Now there is an active interest for semantic pixel-wise labelling. However, some of these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, appear coarse. This is primarily because max pooling and sub-sampling reduce feature map resolution. Our motivation to design SegNet arises from this need to map low resolution features to input resolution for pixel-wise classification. This mapping must produce features which are useful for accurate boundary localization 用深度学习的方法逐像素地分类，目前的算法得到的结果还比较粗糙，主要是最大池化层和下采样造成特征图分辨率的下降。SegNet核心是为了从低分辨率特征图到输入分辨率的映射，这种映射必须产生精确的边界定位。 Of these, the appropriate decoders use the max-pooling indices received from the corresponding encoder to perform non-linear upsampling of their input feature maps.Reusing max-pooling indices in the decoding process has several practical advantages; (i) it improves boundary delineation , (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as FCN , with only a little modification. 好处有三个，增强边界划分能力、减少端到端的训练参数、移植能力强 Most recent deep architectures for segmentation have identical encoder networks, i.e VGG16, but differ in the form of the decoder network, training and inference. Another common feature is they have trainable parameters in the order of hundreds of millions and thus encounter difficulties in performing end-to end training . The difficulty of training these networks has led to multi-stage training , appending networks to a pre-trained architecture such as FCN , use of supporting aids such as region proposals for inference, disjoint training of classification and segmentation networks and use of additional training data for pre-training or for full training. In addition, performance boosting post-processing techniques have also been popular. Although all these factors improve performance on challenging benchmarks , it is unfortunately difficult from their quantitative results to disentangle the key design factors necessary to achieve good performance. We therefore analysed the decoding process used in some of these approaches and reveal their pros and cons 有一些网络训练参数达到百万级别，通常训练的需要额外的辅助。多阶段训练、区域建议、分类和分割网络不相交训练。 We evaluate the performance of SegNet on two scene segmentation tasks, CamVid road scene segmentation and SUN RGB-D indoor scene segmentation 使用CamVid 道路场景分割和SUN RGB-D室内场景分割作为基准。 架构 训练 In order to perform a controlled benchmark we used the SGD solver with a fixed learning rate of 10-3 and momentum of 0.9. The optimization was performed for more than 100 epochs through the dataset until no further performance increase was observed. Dropout of 0.5 was added to the end of deeper convolutional layers in all models to prevent overfitting. For the road scenes which have 11 classes we used a mini-batch size of 5 and for indoor scenes with 37 classeswe used a mini-batch size of 4. 输入尺寸都为360*480。使用SGD，固定的学习率1e-3和0.9的动量，epoch 超过100，直到不再变化。对于11分类采用batchsize=5、37分类batchsize=4。 我们测试了SegNet、FCN、DeepLab-LargFOV、DeconvNet。 分析 To compare the quantitative performance of the different decoder variants, we use three commonly used performance measures: global accuracy (G) which measures the percentage of pixels correctly classified in the dataset, class average accuracy (C) is the mean of the predictive accuracy over all classes and mean intersection over union (mIoU) over all classes as used in the Pascal VOC12 challenge. The mIoU metric is a more stringent metric than class average accuracy since it penalizes false positive predictions. However, mIoU metric is not optimized for directly through the class balanced cross-entropy loss. 分析指标主要是全局精度、平均分类精度、平均交并比。平均交并比是一个更严格的指标，它不是通过交叉熵直接优化的。 note that The mIoU metric does not always correspond to human qualitative judgements (ranks) of good quality segmentation. They show with examples that mIoU favours region smoothness and does not evaluate boundary accuracy 平均交并比更倾向于平滑的边界。 The key idea in computing a semantic contour score is to evaluate the F1-measure which involves computing the precision and recall values between the predicted and ground truth class boundary given a pixel tolerance distance. We used a value of 0.75% of the image diagonal as the tolerance distance. The F1- measure for each class that is present in the ground truth test image is averaged to produce an image F1-measure. Then we compute the whole test set average, denoted the boundary F1-measure (BF) by average the image F1 measures. boundary F1-measure (BF) 指标可以衡量边界轮廓。 This shows that using a larger decoder is not enough but it is also important to capture encoderfeature map information to learn better, particular the fine grained contour information 增加解码网络有助于结果精度的提高，尤其是边缘提取。 道路场景分割 输入尺寸都为360*480。使用SGD，固定的学习率1e-3和0.9的动量，batchsize=5 增大训练样本可以提高平均分类精度和平均交并比。 在CamVid数据集中，训练迭代次数在40K、80K和超过80K，对于最后一次，训练直到精度不再提高或者出现过拟合现象。 Here we note also that over-fitting was not an issue in training these larger models。 过拟合现象不需要我们着重要考虑的。 SegNet G mIOU BF 3.5K dataset-traning -140K 90.40 60.10 46.84]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
        <tag>文献</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PYtorch之torch.nn]]></title>
    <url>%2F2018%2F12%2F13%2FPYtorch%E4%B9%8Btorch.nn%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[PyTorch基础，主要解释torch.nn中的函数方法|卷积网络学习 [TOC] Moduleadd_module(*name*, *module*)添加子模块 apply(*fn*)为所有子模块添加函数 children()返回当前子模块的迭代器 named_children() modules()返回所有子模块 named_modules() parameters(*recurse=True*)返回参数迭代器 named_parameters(*prefix=&#39;&#39;*, *recurse=True*)prefix用来加前缀 cpu()/cuda(*device=None*) eval()/train() double()/float()/half load_state_dict(*state_dict*, *strict=True*) state_dict(*destination=None*, *prefix=&#39;&#39;*, *keep_vars=False*) torch.nn.Module 所有类都要继承的基类 123456789101112import torch.nn as nnimport torch.nn.functional as Fclass Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) add_module 添加子模块 1add_module(name, module) apply(fn) 将函数引用到所有子模块 12345678&gt;&gt;&gt; def init_weights(m): print(m) if type(m) == nn.Linear: m.weight.data.fill_(1.0) print(m.weight)&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))&gt;&gt;&gt; net.apply(init_weights) 迭代器123456789&gt;&gt;&gt; for idx, m in enumerate(net.modules()): #模块迭代器&gt;&gt;&gt; for param in model.parameters(): #参数迭代器&gt;&gt;&gt; for child in model.children(): #子模块迭代器&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):&gt;&gt;&gt; for name, param in self.named_parameters():&gt;&gt;&gt; for name, module in model.named_children(): &gt;&gt;&gt; if name in [&apos;conv4&apos;, &apos;conv5&apos;]: #返回的name的一个用法&gt;&gt;&gt; print(module) parameters Parameters data (Tensor） requires_grad (bool, optional) type(dst_type)Sequential 一个时序容器 Modules按顺序传入 或者传入OrderDict 123456789101112131415# Example of using Sequentialmodel = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() )# Example of using Sequential with OrderedDictmodel = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ])) ModuleList 相当于创建一个list 可以使用append(module)和extend(modules)和insert(index, module)修改已有的list append(module) 将module加到队尾 extend(modules) 将modules组加到队尾 insert(index, module) 插入module ModuleDict 相当于创建一个Dict 功能同ModuleList clear() items() keys() values() pop(key) update(parameters) 还有类似的ParameterList和ParameterDict 卷积网络2D卷积层|Conv2d1torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 输出维度公式 Shape: Input$$(N, C_{in}, H_{in}, W_{in})$$ Output$$(N,C_{out},H_{out},W_{out})$$ $$H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding} -\text{kernel_size} }{\text{stride}} + 1\right\rfloor$$ Note数据的最后一列可能会因为kernal大小设定不当而被丢弃（大部分发生在kernal大小不能被输入整除时，适当的padding可以避免这个问题） 2D反卷积层|ConvTranspose2d1torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1) 输出维度公式$$H_{out} = (H_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{kernel_size} + \text{output_padding}$$ 2D最大池化层|MaxPool2d123456torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)return_indices True时返回最大值的序号，用于上采样ceil_mode 默认向下取整模式#常用kernel_size=stride，same核MaxPool2d(kernel_size=2, stride=2, padding=0) #缩小一倍 输出维度公式$$H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding} - \text{kernel_size} }{\text{stride}} + 1\right\rfloor$$ 2D反池化层|MaxUnpool2d1234torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0, output_size)#参数参照池化层选取MaxUnpool2d(x4pool, id4, kernel_size=2, stride=2, output_size=size4) input input: 输入Tensor indices: 从MaxPool2d得到的最大值序号 output_size (optional): 想要的输出维度 2D平均池化层|AvgPool2d1torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) 2D标准化层|BatchNorm2d12345torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)num_features=C #通道数affine=True #添加可学习的仿射变换参数track_running_stats=True #用于验证预测 $$y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$$ 与InstanceNorm2d的比较BN作用于一个Batch IN作用于单张图片 因此，在特征提取分类等任务中，需要不同图片的联动，选BN 在生成式类任务如风格迁移等，每张照片有自己的style，选IN 超分辨卷积|PixelShufflesub-pixel convolution 1torch.nn.PixelShuffle(upscale_factor) 将低分辨率图片变为高分辨率图片$$ (*, C \times r^2, H, W)—-&gt; (C, H \times r, W \times r)$$ 2D双线性上采样|UpsamplingBilinear2d123torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)size=(H_out, W_out) 损失函数|Loss functionsL1Loss MSELoss CrossEntropyLoss NLLLoss|NLLLoss2d]]></content>
      <categories>
        <category>python</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VGG读书笔记]]></title>
    <url>%2F2018%2F12%2F11%2FVGG%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[经典网络VGG，读书笔记，文献翻译 [TOC] 摘要 本文主要研究卷积神经网络深度对准确率的影响。利用3*3的小卷积核加大网络的深度。结果表明加深网络深度至16-19层可以极大地提高准确率。同时模型对其他数据集也有很好的泛化能力。 引言 ILSVRC 比赛大大促进了deep visual recognition architectures 的发展。 In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers. 本文主要解决了网路的深度问题。固定其他超参数，通过增加更多的3*3的小卷积核加深网络深度。 2 卷积神经网络配置大量使用3*3的小卷积核。应为它是捕捉左/右/上/下、中心的最小核。 2.1 架构（ARCHITECTURE ） 输入为224*224的RGB图片。 预处理为减去每一个像素RGB的均值。 后面是一系列的卷积层，使用3*3的小卷积核。 后面是全连接层，两个4096通道，后面一个1000通道分类器。 2.2 配置 下图列出了A-E，5中网络深度随之增大。A有8个卷积网络和3个全连接层；E有16个卷积网络和3个全连接层。通道数从64开始，512结束。 2.3 讨论 VGG使用小卷积核的原因是：两个3*3的卷积层和一个5*5的卷积层具有相同的感受野；三个3*3的卷积层和一个7*7的卷积层具有相同的感受野。 3.1 训练训练超参数123456batchsize=256,使用MBGD(mini-batch gradient descent)梯度下降,lr=0.01 每当精度不再下降时 lr=lr*0.1monentum=0.9 Drop(0.5)weight decay L2=5e-4最终结果lr--&gt;1e-5,iterations=370K(epochs=74) 数据集处理 预处理包括 ： 裁剪尺寸=224*224，图片随机水平翻转和随机RBG颜色偏移 训练尺寸图片尺寸应&gt;=224 我们考虑了两种设置训练尺寸S的方法： 第一种固定尺寸。我们设置S=256和S=384。为了加速S=384的训练，我们使用小学习率lr=1e-3 第二种设置多尺度训练，每一幅图片随机裁剪S–&gt;[Smin,Smax]，(256,512)。可以看成是缩放抖动。 3.2 测试 将全连接层换成卷积层。第一个全连接层转换为7*7的卷积；第二第三个转换为1*1的卷积。这样是为了适应输出不同尺寸的图片的测试结果。最后累加得到1*1*num_classes的结果。 4 分类实验 数据集使用ILSVRC-2012。数据集包含1000个类。训练图片1.3M，交叉验证50K，测试图片100K。有两种测试指标top-1和top-5。 top5是指给出5个分类结果，有一个对即可。 4.1 单尺寸评估 单尺寸（S = 256 or S = 384 ） 首先，LRN网络对提高精度没有帮助。 其次，提高网络深度有助于提高精度，同时使用3*3卷积核比使用1*1的卷积核要有效果。使用5*5的卷积核效果也没有更好。 最后，尺寸抖动技术有更好的效果。 深层网络需要使用小卷积核 使用尺寸抖动技术better 4.2多尺寸评估4.3 Multi-crop 评估 Multi-crop evaluation：对图像进行随机裁剪，然后通过网络预测每一个样本的类，最后对所有结果求平均 Dense evaluation：利用FCN思想，改全连接层为卷积层，将原图送入网络预测结果。得到score map，最后求平均。 Multi-crop相当于padding补充0。better Dense evaluation相当于padding补充相邻像素点，并增大了感受野。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
        <tag>文献</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch模型保存与载入]]></title>
    <url>%2F2018%2F11%2F28%2FPytorch%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%BD%BD%E5%85%A5%2F</url>
    <content type="text"><![CDATA[Pytorch基础，主要是保存模型数据，载入模型数据 [TOC] pytorch保存数据1234567state=&#123;'net':model.state_dict(),'optimizer':optimizer.state_dict(),'epoch':Epoch&#125;dir='model.pth'torch.save(state,dir)#或者# torch.save(cnn1,'cnn1.pkl') #保存整个网络结构与参数# torch.save(cnn1.state_dict(),'cnn1_params.pkl') #只保存参数 pytorch载入123456789dir='model.pth'state=torch.load(dir)model.load_state_dict(state['net'])optimizer.load_state_dict(state['optimizer'])Epoch=state['epoch']#或者net=Net()net.load_state_dict(torch.load('net.pth'))]]></content>
      <categories>
        <category>python</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch加载数据集]]></title>
    <url>%2F2018%2F11%2F28%2FPyTorch%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[PyTorch基础，主要是数据的加载和保存 [TOC] Dataset类 在PyTorch中，加载数据集，需要继承torch.utils.data.dataset 中的Dataset类重写方法 12&gt; from torch.utils.data.dataset import Dataset&gt; 12345678class dataset(Dataset): def __init__(self, ...): def __getitem__(self, ...): return (...) def __len__(self, ...): return len(...) 一个读取图像数据集的例子 1234567891011121314151617181920212223242526272829import osfrom PIL import Imageclass DatasetFromjpg(Dataset): def __init__(self, root, transforms=None): """ Args: root (string): 文件路径 transform: transform 操作 """ self.root=root imgs=os.listdir(root) #这里只是加载路径 self.imgs=[os.path.join(root,img) for img in imgs] #list[图片路径] self.transforms = transforms def __getitem__(self, index): #cat.10.jpg/dog.20.jpg #dog-&gt;1 cat-&gt;0 img_path=self.imgs[index] label=1 if 'dog' in img_path.split('/')[-1] else 0 pil=Image.open(img_path) if self.transforms is not None: img_as_tensor = self.transforms(pil) return (img_as_tensor, label) else: img_as_np = np.asarray(pil) return (img_as_np,label) def __len__(self): return len(self.imgs) transforms 针对PIL图片常用函数 12&gt; from torchvision import transforms&gt; 1234567891011#有次序！！！transformations = /transforms.Compose([transforms.Resize(224), #调整大小，保持比例 最小边-224 transforms.CenterCrop(234), #中心裁剪 transforms.RandomCrop, # transforms.RandomSizedCrop, # transforms.Pad #填充 transforms.ToTensor(), transforms.ToPILImage(), transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5]) ]) # 将读取的图片变化 DataLoader类 加载数据 12&gt; from torch.utils.data import DataLoader&gt; 1234567891011121314train_loader=DataLoader(dataset=dataset, batch_size=10, shuffle=True, #乱序 num_workers=2, #CPU多线程 sample #样本抽样 )#DataLoader是一个迭代对象from torchvision.utils import make_grid,save_imagedataiter = iter(train_loader)images,labels=next(dataiter)photos=make_grid(imgs,3) #tensor类型 3*3网格# plt.imshow(photos)save_image(photos,'save_photos.jpg')]]></content>
      <categories>
        <category>python</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch学习]]></title>
    <url>%2F2018%2F11%2F28%2FPyTorch%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[机器学习基础，主要是简单的创建数组、算数操作、类型转换等 [TOC] 注意123#inplace方式a.add(b) a.add_(b) #会改变自身数据 函数名以_结尾的都是inplace方式 基础操作创建数组123456789101112#创建Tensortensor([list]) 类似np.array的构造函数 ones(*sizes) 全1Tensor zeros(*sizes) 全0Tensor eye(*sizes) 对角线为1，其他为0 arange(s,e,step) 从s到e，步长为step linspace(s,e,steps) 从s到e，均匀切分成steps份 rand/randn(*sizes) 均匀/标准分布 normal(mean,std) 正态分布/均匀分布 randperm(m) 随机排列 #a.size() 是tuple的子类 Tensorlist/numpy123456#Tensor&lt;---&gt;lista.tolist()#Tensor&lt;-----&gt;numpytorch.from_numpya.numpy() 算术操作123456789101112131415161718192021222324252627282930torch.view #=reshapetorch.unsqueeze(d) #增加维度 从0开始torch.squeeze(d) #压缩为1的维度torch.mm #矩阵乘法t #转置 clamp(input, min, max) #超过min和max部分截断 #逐元素操作abs/sqrt/div/exp/fmod/log/pow.. #绝对值/平方根/除法/指数/求余/求幂.. cos/sin/asin/atan2/cosh #相关三角函数 ceil/round/floor/trunc #上取整/四舍五入/下取整/只保留整数部分 sigmod/tanh #激活函数 #归并操作mean/sum/median/mode #均值/和/中位数/众数 norm/dist #范数/距离 std/var #标准差/方差 cumsum/cumprod #累加/累乘 #线性代数trace #对角线元素之和(矩阵的迹) diag #对角线元素 triu/tril #矩阵的上三角/下三角，可指定偏移量 mm/bmm #矩阵乘法，batch的矩阵乘法 addmm/addbmm/addmv/addr/badbmm #矩阵运算 dot/cross #内积/外积 inverse #求逆矩阵 svd #奇异值分解 Tensor类型转换 Data type dtype Tensor types 32-bit floating point torch.float32 or torch.float torch.*.FloatTensor 64-bit floating point torch.float64 or torch.double torch.*.DoubleTensor 16-bit floating point torch.float16 or torch.half torch.*.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.*.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.*.LongTensor 123456b=a.type(torch.FloatTensor)tensor.to(torch.float64)#CPU---&gt;GPUtensor.cuda()tensor.cpu() 保存与载入12torch.save(a,'a.pth')torch.loas(a,'a.pth') Variable 123data tensor类型grad 梯度creator 模型相关12345678#针对dropoutmodel.training() #表明在train模式model.eval() #针对测试torch.save(net.state_dict(),'net.pth')net=Net()net.load_state_dict(torch.load('net.pth'))]]></content>
      <categories>
        <category>python</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn回顾]]></title>
    <url>%2F2018%2F11%2F15%2FSklearn%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[简单回顾了Sklearn的基本操作 Sklearn回顾数据输入1234from sklearn.cross_validation import train_test_splitX_train,X_test,Y_train,Y_test=train_test_split(X,y, train_size=0.7, random_state=1) 交叉验证！图片 12from sklearn.cross_validation import cross_val_scorecross_val_score(model,X,y,cv=5) 混淆矩阵！图片 1234import seaborn as snsfrom sklearn.metrics import confusion_matrixmat=confusion_matrix(Y_test,y_pred)sns.heatmap(mat,square=True,annot=True,cbar=False) 简单线性回归1234567891011121314151617181920212223#导入数据from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y, train_size=0.7, random_state=1)X_train=X_train.valuesy_train=y_train.values#2.导入模型from sklearn.linear_model import LinearRegressionmodel=LinearRegression(fit_intercept=True) #设置截距#3.模型适配（训练）model.fit(X_train,y_train)#4.可以查看参数矩阵model.coef_,model.intercept_#5.预测y_pred=model.predict(X_test)#6.评估from sklearn.metrics import accuracy_scoreaccuracy_score(y_test,y_pred)#(7.混淆矩阵)from sklearn.metrics import confusion_matrixmat=confusion_matrix(y_test,y_pred)sns.heatmap(mat,square=True,annot=True,cbar=False) 简单高斯贝叶斯 123from sklearn.naive_bayes import GaussianNBmodel=GaussianNB()model.fit(X_train,y_train)]]></content>
      <categories>
        <category>python</category>
        <category>Sklearn</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib回顾]]></title>
    <url>%2F2018%2F11%2F13%2FMatplotlib%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[简单回顾了Matplotlib的基本操作 Matplotlib回顾开始使用123456789%matplotlib inline#%matploylib notebookimport matplotlib as mplimport matplotlib.pyplot as pltplot.show() #一个session只能使用一次，通常在末行开启fig.savefig('my_photo.png') #保存图片 简易线性图—&gt;颜色/坐标轴/标签12345678910111213141516171819202122232425262728fig=plot.figure()ax=plt.axes()#直线x=np.linspace(0,10,1000)plt.plot(x,np.sin(x))#颜色pltpltpltplt.plot(x,np.sin(x-0)，color='blue')pltpltplt.plot(x,np.sin(x-1),color='g') #rgbcmykpltplt.plot(x,np.sin(x-2),color='0.75') #灰度：0-1plt.plot(x,np.sin(x-3),color='#FFFFDD')#元素plt.plot(x,np.sin(x-0)，linestyle='-')# '-' '--' '-.' ':'#颜色+元素 ‘--g’#坐标轴plt.xlim(-1,1) #正序plt.ylim(10,1) #逆序plt.axis('tight') #自动收缩空白区域#标签titlexlabel ylabellegend 散点图123456789101112131415161718192021222324plt.plot(x,y, 'o' ,color='r') #'o' '.' ',' 'x' '+' 'v' '^' '&lt;' '&gt;' 's' 'd'#一步到位ptl.plot(x,y,'-ok') #直线(-) 圆圈(o) 黑色(k)#配置参数plt.plot(x,y,'-ok', markersize=15, #点大小 linewidth=4, #线宽 markerfacecolor='white', #中心点颜色 markeredgewidth=2 #点边缘 )plt.scatter(x,y,marker='o')#不同颜色深浅的散点图rng=np.random.RandomState(0)x=rng.randn(100)y=rng.randn(100)colors=rng.rand(100)sizes=rng.rand(100)*1000plt.scatter(x,y,c=colors,s=sizes,alpha=0.4,cmap='viridis')plt.colorbar() #颜色栏 三维图像12345678910111213141516x=np.linspace(0,5,50)y=np.linspace(0,5,40)X,Y=np.meshgrid(x,y)Z=np.sin(X)**10+np.cos(10+X*Y)+np.cos(Y)plt.contour(X,Y,Z, 20, #将数据范围等分20 cmap='RdGy' )plt.contourf()#!!!!!!!!!!!!左上角(0,0)plt.imshow(Z, extent=[0,5,0,5], #坐标轴 origin='lower', # cmap='RdGy') 统计特性—-&gt;直方图/联合分布/seaborn123456789101112131415161718192021222324252627282930#一维plt.hist(data, bins=30, normed=True, alpha=0.5, )counts,bin_edges=np.histogram(data,bins=30) #只输出结果#二维plt.hist2dplt.hexbin #正六边形np.histhistogram2dimport seaborn as sns #使用基于matpltlib的高级APIsns.histsns.kdeplot(x,shade=True) #直方图平滑#hist和KDE相结合sns.distplot#二维联合分布sns.jointplot(x,y,kind='kde') #二维kde#sns.jointplot('x','y',data=DataFrame,kind='hex')sns.jointplot(x,y,kind='hex') sns.jointplot(x,y,kind='reg') #带回归的联合分布 #矩阵图 多变量两两关系图sns.pairplot(DataFrame,hue='species',size=2.5)]]></content>
      <categories>
        <category>python</category>
        <category>Matplotlib</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas回顾]]></title>
    <url>%2F2018%2F11%2F13%2FPandas%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[简单回顾了Pandas的基本操作 Pandas回顾[TOC] Series123456789101112131415161718#字典-数组Series=pd.Series([11,12,13,14]) #直接式#Series=pd.Series(&#123;0:11,1:12,2:13,3:14&#125;) #字典式index=[0,1,2,3]Series=pd.Series(data,index=index) #索引Series#index values#0 11#1 12#2 13#3 14#dtype: int64#有index values dtype #数据索引Series.loc 显式从1开始Series.iloc 隐式 从0开始，左闭右开 DataFrame123456789101112131415#字典-Seriesdata=pd.DataFrame(&#123;'col1':Series1, 'col2':Series2 &#125;)data#index col1 col2#Califor 11 12#Flori 12 12#Texas 2 3#有index columns#索引方式data['col1']data.loc[:,'col1':'col2']#data.ix[:3,'col1':'col2'] 索引对齐12345678910111213141516171819202122A=pd.DataFrame# a b#0 2 3#1 4 5B=pd.DataFrame# a b c#0 1 1 1#1 1 1 1#2 1 1 1A.add(B,fill.value=A.stack().mean())#0 3 4 NaN#1 5 6 NaN#2 NaN NaN NaN #NaN=fill.value+ add- sub* mul/ div// floordiv% mod** pow 缺失值1234567891011isnull #返回marknotnull #与isnull相反dropna(axis，how,thresh) #返回剔除缺失值的数据axis=1 #按维度剔除how=all/any #剔除要求thresh=3 #最小数目fillna(method,axis) #返回填充缺失值的数据副本method=ffill/bfill #继承前/后数值来填充axis #继承方向 高维数据多级索引二维Series12345678910#二维Seriesdata=np.random.randn(4)index=[('a',1),('a',2),('b',1),('b',2)] #元组Series=pd.Series(data,index=index)index=pd.MultiIndex.from_tuples(index) #从元组转换# MultiIndex(levels=[['a', 'b'], [1, 2]],# labels=[[0, 0, 1, 1], [0, 1, 0, 1]])Series=Series.reindex(index) #多级索引data=Seires.unstack() #与DataFrame转换 ---stack 高维DataFrame1234567891011121314151617181920#高维DataFrameoridata=np.random.randn(4,6)index=pd.MultiIndex.from_product([['a','b'],[1,2]], names=['level1','level2'])columns=pd.MultiIndex.from_product([['data1','data2','data3'],['temp1','temp2']], names=['label1','label2'])data=pd.DataFrame(data=oridata,index=index,columns=columns) #两个索引的笛卡尔积data# label1 data1 data2 data3# label2 temp1 temp2 temp1 temp2 temp1 temp2#level1 level2#a 1# 2#b 1# 2#数据索引idx=pd.IndexSlicedata.loc[ idx[:,1] ,idx[:,'temp2'] ] 索引操作12345678对DataFrame的操作sort.index() #方法--排序(字典序)对Series的操作unstack(level=0) #---&gt;二维 stack #反操作reset_index(name='level1') #行列转换 数据集操作123456789101112#简单合并pd.concat( ([DataFrame1,DataFrame2]), axis=0 #合并方向 verity_integrity=True #重复索引触发异常 ignore_index=True #创造新的索引 key=['DataFrame1','DataFrame2'] #增加多级索引 join='inner'/'outer' #列名的交集/合集 )#数据库式合并pd.merge([DataFrame1,DataFrame2])]]></content>
      <categories>
        <category>python</category>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy回顾]]></title>
    <url>%2F2018%2F11%2F12%2FNumpy%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[简单回顾了Numpy的基本操作 Numpy回顾[TOC] 创建数组（zeros/arange/random/eye)1234567np.zeros( (2,3),dtype=int ) #onesnp.arange(0,20,2) #步长为2np.random.randn(3,4) #random normal randintnp.eye(2,3) 数组性质（ndim/shape/dtype)123456np.random.seed(0)x=np.random.randn(2,3,3)print('x.ndim: ',x.ndim)print('x.shape: ',x.shape)print('x.dtype: ',x.dtype) 副本与副本copy123456x=np.random.randn(4,5)x1=x[:2,:2]x1[:,:]=666; #非副本,会改变原数组的值x2=x[:2,:2].copy() #副本 数组拼接(concatenate/vstack/hstack)123456x=np.arange(4)y=np.arange(3)z=np.concatenate([x,y]) #np.vstack np.hstack#np.concatenate([x,y],axis=0) 高维拼接 通用算术1234567// #取整除数**%abssin #弧度expm1 #针对小数值log1p #针对小数值 scipy.special统计特性123456789101112sum #对整个数组的聚合sum(axis=0) #对单一维度进行折叠---相当于对合并行prod #积mean #均值std #标准差var #方差argmin #最小值索引 argmaxmedian #中位数percentile #统计值 percentile(a,25)--分位数any #只要有一个为Trueall 布尔运算符12345#逐位&amp;|^ #xor~ 广播123#规则1：两个数组维度不一样，那么小的数组左边补1#规则2：两个数组维度不匹配，则扩展数值=1的维度#否则引发错误 索引123456789101112#下标传递x=[-1,0,1,2,3,4,5]ind=[2,0,1]x[ind]=[1,-1,0]x=[ [0,1,2] [3,4,5] [6,7,8]]row=[0,1,2]col=[2,1,3]x[row,col].shape=(3,) #第一个索引为下标1，第二个索引为下标2#本质是先广播，后取值 排序1234567sort #=sort 默认从小到大sort(x,axis=0) #会丢失数据之间的依赖关系argsort #=sortedpartition #部分排序partition(x,3,axis=0) #前三个位置为最小的三个值]]></content>
      <categories>
        <category>python</category>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中cout输出格式]]></title>
    <url>%2F2018%2F09%2F18%2FC%2B%2B%E4%B8%ADcout%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[C++中cout的日常使用 控制输出格式 ##控制格式输出 使用样例123456789101112#include &lt;iostream&gt; //使用流对象成员函数#include &lt;iomanip&gt;//使用控制符设置using namespace std;int main()&#123; int a = 31; cout.width(8); //使用流对象成员函数 cout.fill(&apos;*&apos;); cout.unsetf(ios::dec); cout.setf(ios::hex); cout &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; setw(8) &lt;&lt; setfill(&apos;*&apos;) &lt;&lt; resetiosflags(ios::dec) &lt;&lt; setiosflags(ios::hex) &lt;&lt; a &lt;&lt; endl; //使用控制符设置&#125; 参考自乌托的博客]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>cout</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++复习一：语言基础]]></title>
    <url>%2F2018%2F09%2F18%2FC%2B%2B%E5%A4%8D%E4%B9%A0%E4%B8%80%EF%BC%9A%2F</url>
    <content type="text"><![CDATA[算术类型 变量初始化 const|constexpr auto|decltype 类型转换 算术类型 布尔类型 bool 字符类型 char 整数类型 int / long 浮点类型 float / double 用 sizeof求解占用的内存字节数带符号 signed不带符号 unsigned 变量初始化123int count=0; //拷贝初始化int count(0); //直接初始化int count=&#123;0&#125;; //列表初始化 左值和右值 const|constexpr与auto|decltypeconst 常量constexpr 常量表达式12const int size=20；//运行时常量constexpr int limits=size+20;//编译技术时会是一个常数 auto 自动类型说明符decltype 类型指示符123auto x=1.0; //编译时用正确的类型替换autodecltype(ci) x=1; //只定义x为ci的类型，不赋初值decltype(f() ) y=sum; //定义y为f()的返回值类型 类型转换static_cast用于将一种数据类型转换成另一种数据类型，使用格式如下：变量1 = static_cast&lt;变量1数据类型&gt;(另外一种数据类型变量或表达式);123int a = 1;float b;b = static_cast&lt;float&gt;(a); //类似于C语言中的b = (float)a; const_cast去除表达式中的const限定。用于指针、引用、变量123const int i=0;int *j=&amp;i; //错误j=const_cast_&lt;int *&gt;(&amp;i); //正确 dynamic_cast执行派生类指针或引用与基类指针或引用之间的转换。 reinterpret_cast 不常用从字面意思理解是一个“重新解释的类型转换”。也就是说对任意两个类型之间的变量我们都可以个使用reinterpret_cast在他们之间相互转换，无视类型信息。 按位运算符按位与运算符（&amp;）按位或运算符（|）异或运算符（^）取反运算符（~）左移运算符（&lt;&lt;） 右补0右移运算符（&gt;&gt;） 左补0 不同长度的数据进行位运算如果两个不同长度的数据进行位运算时，系统会将二者按右端对齐，然后进行位运算。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客建站]]></title>
    <url>%2F2018%2F09%2F16%2F%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%2F</url>
    <content type="text"><![CDATA[写在前面 早就想着用Github用建个个人博客，刚开始试了官方推荐的jekyll，后来发现好多人多用hexo建站。我两个都试过了，觉得还是hexo好用关键是hexo官方有中文文档。 目前的配置是Github部署网站+Hexo建站工具+next主题+HexoEditor编辑器+腾讯云图床 1.下载安装 下载github安装，其中自带了git。 在github官网申请注册帐号，需要用到邮箱，注册后一定要记住三个东西：你的用户名(username)，你的密码(password)，以及你的邮箱(email) 安装Node.js 新建hexo文件夹，打开 Git Bash。接下来的命令均在Git Bash中执行 安装 Hexo : $npm install -g hexo 安装依赖包： $npm install 执行$hexo init 初始化网站这时候用hexo s就能预览效果了 新建Github仓库：仓库名必须为你的Github名.github.io，要不然就不能使用Github Pages服务了。。。 2.配置 SSH 本地生成公钥私钥 $ssh-keygen -t rsa -C &quot;你的邮件地址&quot; 添加公钥到 Github 根据上一步的提示，找到公钥文件（默认为id_rsa.pub），用记事本打开，全选并复制。 登录 Github，右上角 头像 -&gt; Settings —&gt; SSH keys —&gt; Add SSH key。把公钥粘贴到key中，填好title并点击 Add key。 git bash中输入命令$ssh -T git@github.com，选yes，等待片刻可看到成功提示。 3. NexT主题下载 NexT 主题是由 iissnan 大神所制作的一款简洁美观不失逼格的主题。下载方法有以下两种： 进入博客根目录/themes/, 执行$git clone https://github.com/iissnan/hexo-theme-next.git 直接进入上面的链接，在项目主页download zip文件，然后解压到博客根目录/themes/ 文件夹4. 发布使用以下两条命令进行发布，发布成功后可在浏览器中使用你的github名.github.io进入你的博客~ 12$hexo clean$hexo d -g Hexo 常用命令1.本地预览：12$hexo server//或 hexo s//然后打开浏览器输入localhost:4000可以预览博客效果，用于调试 2. 新建文章12$hexo new post &quot;title&quot;//新文章位置：/source/_posts 3. 新建页面1$hexo new page &quot;title&quot; 4. 部署并生成1$hexo d -g 5. 清除生成的文件和缓存1$hexo clean 推荐使用HexoEditor编辑器就不用手打命令了。4. next主题5. HexoEditorHexoEditor下载地址：https://github.com/zhuzhuyule/HexoEditor里面有详细安装说明Tips12//If find Error about download Electron faild ,you can try run thosenpm install -g electron@1.8.1 6. 腾讯云图床参考资料 Hexo3.1.1静态博客搭建指南 NexT使用文档 Hexo官方文档（中文版） Hexo官方文档（中文版）]]></content>
      <categories>
        <category>hexo</category>
        <category>next</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建图床]]></title>
    <url>%2F2018%2F09%2F15%2F%E5%88%9B%E5%BB%BA%E5%9B%BE%E5%BA%8A%2F</url>
    <content type="text"><![CDATA[新浪微博相册图片支持外链，速度快，免费。用新浪微博相册可以为博客、写作平台打造一个免费的图床。 也可以使用腾讯云的对象储存，个人实名以后每个月有大量的免费额度，供个人博客使用足够了。 微博做图床 注册微博账号 上传图片到微博相册 获取外链：点击图片-查看大图-右键图片复制图片链接，得到如下链接： 1http://ww1.sinaimg.cn/large/sample.jpg 同时还有其他大小可以选择1234http://ww1.sinaimg.cn/thumbnail/sample.jpg // 缩略图http://ww1.sinaimg.cn/small/sample.jpg // 稍微大点的图http://ww1.sinaimg.cn/bmiddle/sample.jpg // 再大点的图http://ww1.sinaimg.cn/large/sample.jpg // 最大的 推荐使用picgo图库神器上传图片 腾讯云oss做图床 注册腾讯云账号网上也有推荐七牛云做图床的。但是要上传手持身份证照片。我嫌麻烦。腾讯云只用拿微信做个验证就好了。 创建储存桶。注意有两个关键的配置不能忽略存储桶（bucket）访问权限防盗链设置 访问权限访问权限应设置为公有读私有写。因为我们要去读照片嘛。 防盗链把自己的博客地址加进去 使用腾讯云提供的cos客户端上传图片。右键点击获取图片链接。注意：如果没有把本地地址加入白名单，是不能看到预览图的。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
