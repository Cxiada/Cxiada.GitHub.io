<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Cxiada&#39;s blog</title>
  
  <subtitle>职业码农</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://Cxiada.github.io/"/>
  <updated>2018-12-24T03:34:09.063Z</updated>
  <id>https://Cxiada.github.io/</id>
  
  <author>
    <name>xdChen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pytorch学习之优化器（optim）</title>
    <link href="https://Cxiada.github.io/2018/12/24/Pytorch%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88optim%EF%BC%89/"/>
    <id>https://Cxiada.github.io/2018/12/24/Pytorch学习之优化器（optim）/</id>
    <published>2018-12-24T04:00:00.000Z</published>
    <updated>2018-12-24T03:34:09.063Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch基础，主要解释torch.optim中的函数方法|优化方法</p><a id="more"></a>  <p>[TOC]</p><h4 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h4><blockquote><p>为了构建一个<code>Optimizer</code>，你需要给它一个包含了需要优化的参数（必须都是<code>Variable</code>对象）的iterable。然后，你可以设置optimizer的参 数选项，比如学习率，权重衰减，等等。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr = <span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer = optim.Adam([var1, var2], lr = <span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure><h4 id="为特定层单独设置参数"><a href="#为特定层单独设置参数" class="headerlink" title="为特定层单独设置参数"></a>为特定层单独设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optim.SGD([&#123;<span class="string">'params'</span>:model.base.parameters()&#125;,</span><br><span class="line">           &#123;<span class="string">'params'</span>:model.base2.parameters(), <span class="string">'lr'</span>:<span class="number">1e-3</span>&#125;]</span><br><span class="line">         ,lr=<span class="number">1e-2</span>,momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h4 id="执行优化器"><a href="#执行优化器" class="headerlink" title="执行优化器"></a>执行优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss=loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h4 id="载入与保存"><a href="#载入与保存" class="headerlink" title="载入与保存"></a>载入与保存</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">state_dict()</span><br><span class="line"></span><br><span class="line">load_state_dict(state_dict)</span><br></pre></td></tr></table></figure><p>常用优化算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>, betas=(<span class="number">0.9</span>,<span class="number">0.999</span>), weight_decay=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr = <span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h4 id="改变学习率"><a href="#改变学习率" class="headerlink" title="改变学习率"></a>改变学习率</h4><h5 id="手动改变学习率"><a href="#手动改变学习率" class="headerlink" title="手动改变学习率"></a>手动改变学习率</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param[<span class="string">'lr'</span>] = <span class="number">1e-4</span></span><br></pre></td></tr></table></figure><h5 id="自定义变学习率"><a href="#自定义变学习率" class="headerlink" title="自定义变学习率"></a>自定义变学习率</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Assuming optimizer has two groups.</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lambda1 = <span class="keyword">lambda</span> epoch: epoch // <span class="number">30</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lambda2 = <span class="keyword">lambda</span> epoch: <span class="number">0.95</span> ** epoch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    scheduler.step()</span><br></pre></td></tr></table></figure><h5 id="固定步长变学习率"><a href="#固定步长变学习率" class="headerlink" title="固定步长变学习率"></a>固定步长变学习率</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.optim.lr_scheduler.StepLR(optimizer, step_size,gamma=<span class="number">0.1</span>,last_epoch=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Assuming optimizer uses lr = 0.05 for all groups</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># lr = 0.05     if epoch &lt; 30</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># lr = 0.005    if 30 &lt;= epoch &lt; 60</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># lr = 0.0005   if 60 &lt;= epoch &lt; 90</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># ...</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>scheduler = StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    scheduler.step()</span><br></pre></td></tr></table></figure><h5 id="自适应变学习率"><a href="#自适应变学习率" class="headerlink" title="自适应变学习率"></a>自适应变学习率</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#当精度不再提高时，变学习率</span></span><br><span class="line"></span><br><span class="line">torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">'min'</span>, factor=<span class="number">0.1</span>, patience=<span class="number">10</span>, verbose=<span class="keyword">False</span>, threshold=<span class="number">0.0001</span>, cooldown=<span class="number">10</span>, min_lr=<span class="number">0</span>, eps=<span class="number">1e-08</span>)</span><br><span class="line"></span><br><span class="line">modemode=<span class="string">'min'</span>/<span class="string">'max'</span>  </span><br><span class="line">factor=<span class="number">0.1</span>   学习率变化因子</span><br><span class="line">patience=<span class="number">10</span>  如果<span class="number">10</span>次没有检测到精度提高，则改变学习率</span><br><span class="line">verbose=<span class="keyword">False</span>   <span class="comment">#改变学习率同时打印</span></span><br><span class="line">threshold=<span class="number">0.0001</span>   <span class="comment">#比较精度</span></span><br><span class="line">cooldown=<span class="number">10</span>         <span class="comment">#改变学习率后10次内不再变化</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch基础，主要解释torch.optim中的函数方法|优化方法&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Pytorch" scheme="https://Cxiada.github.io/categories/python/Pytorch/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
      <category term="优化方法" scheme="https://Cxiada.github.io/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>FCN读书笔记</title>
    <link href="https://Cxiada.github.io/2018/12/18/FCN%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>https://Cxiada.github.io/2018/12/18/FCN读书笔记/</id>
    <published>2018-12-18T04:00:00.000Z</published>
    <updated>2018-12-18T15:17:10.037Z</updated>
    
    <content type="html"><![CDATA[<p>经典语义分割网络FCN，读书笔记，文献翻译</p><p>参考译文：<a href="https://www.jianshu.com/p/91c5db272725" target="_blank" rel="noopener">用于语义分割的全卷积网络FCN（UC Berkeley</a></p><a id="more"></a>  <p>[TOC]</p><p><img src="E:\GitHub1\blog\source\image\FCN读书笔记\架构.JPG" alt=""></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>卷积网络在特征分层领域是非常强大的视觉模型。我们证明了经过端到端、像素到像素训练的卷积网络超过目前语义分割中最先进的技术。我们的核心观点是建立 “全卷积” 网络，输入任意尺寸，经过有效的推理和学习产生相应尺寸的输出。</p><p>我们改编当前的分类网络(AlexNet ,the VGG net , and GoogLeNet ) 到完全卷积网络。然后我们定义了一个跳跃式的架构，结合来自深、粗层的语义信息和来自浅、细层的表征信息来产生准确和精细的分割。</p></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><blockquote><p>Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies jointly encode location and semantics in a localto-global pyramid. We define a novel “skip” architecture to combine deep, coarse, semantic information and shallow, fine。</p><p>语义分割面临在语义和位置的内在张力问题：<strong>全局信息解决的 “是什么”</strong>，而<strong>局部信息解决的是 “在哪里</strong>”。深层特征通过非线性的局部到全局金字塔编码了位置和语义信息。我们定义了一种利用集合了深、粗层的语义信息和浅、细层的表征信息的特征谱的跨层架构。</p></blockquote><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><blockquote><p>Dense prediction with convnets Several recent works have applied convnets to dense prediction problems…Common elements of these approaches include </p><p>• small models restricting capacity and receptive fields<br>• patchwise training<br>• post-processing by superpixel projection, random field<br>regularization, filtering, or local classification<br>• input shifting and output interlacing for dense output as introduced by OverFeat<br>• multi-scale pyramid processing<br>• saturating tanh nonlinearities  and<br>• ensembles</p></blockquote><p>这些方法的相同点包括如下：</p><ul><li>限制容量和接收域的小模型</li><li>patchwise 训练</li><li>超像素投影的预处理，随机场正则化、滤波或局部分类 </li><li>输入移位和 dense 输出的隔行交错输出 </li><li>多尺度金字塔处理 </li><li>饱和双曲线正切非线性 </li><li>集成 </li></ul><blockquote><p>whereas our method does without this machinery. However, we do study patchwise training and “shift-and-stitch” dense output  from the perspective of FCNs. We also discuss in-network  sampling, of which the fully connected prediction by Eigen et al.  is a special case. </p><p>然而我们的方法确实没有这种机制。但是我们研究了 patchwise 训练 和从 FCNs 的角度出发的 “shift-and-stitch”dense 输出。我们也讨论了网内上采样，其中 Eigen 等人[7] 的全连接预测是一个特例。</p></blockquote><h2 id="3-全卷积网络"><a href="#3-全卷积网络" class="headerlink" title="3 全卷积网络"></a>3 全卷积网络</h2><blockquote><p>Each layer of data in a convnet is a three-dimensional array of size h × w × d, where h and w are spatial dimensions, and d is the feature or channel dimension. The first layer is the image, with pixel size h × w, and d color channels. Locations in higher layers correspond to the locations in the image they are path-connected to, which are called their receptive fields. </p><p>卷积网的每层数据是一个h × w × d 的三维数组，其中 h 和 w 是空间维度, d 是特征或通道维数。第一层是像素尺寸为  h × w、颜色通道数为 d 的图像。高层中的 locations 和图像中它们连通的 locations 相对应，被称为接收域。</p></blockquote><h3 id="3-1-网络改编"><a href="#3-1-网络改编" class="headerlink" title="3.1 网络改编"></a>3.1 网络改编</h3><p>去掉网络中的全连接层，用卷积层代替。</p><p><img src="E:\GitHub1\blog\source\image\FCN读书笔记\图1.png" alt=""></p><h3 id="3-2-Shift-and-stitch-是滤波稀疏"><a href="#3-2-Shift-and-stitch-是滤波稀疏" class="headerlink" title="3.2 Shift-and stitch 是滤波稀疏"></a>3.2 Shift-and stitch 是滤波稀疏</h3><h3 id="3-3-上采样是向后向卷积"><a href="#3-3-上采样是向后向卷积" class="headerlink" title="3.3 上采样是向后向卷积"></a>3.3 上采样是向后向卷积</h3><blockquote><p>Another way to connect coarse outputs to dense pixels is interpolation. </p><p>使用双线性插值可以从粗糙输出到密集像素输出。</p></blockquote><blockquote><p>Note that the <strong><u>deconvolution</u></strong> filter in such a layer need not be fixed (e.g., to bilinear upsampling), but can be learned. A stack of deconvolution layers and activation functions can even learn a nonlinear upsampling. </p><p>需要注意的是<strong><u>反卷积</u></strong>滤波在这一层不需要被固定不变（比如双线性上采样）但是可以被学习。一堆反卷积层和激励函数甚至能学习一种非线性上采样。</p></blockquote><h3 id="3-4-patchwise-训练是一种损失采样"><a href="#3-4-patchwise-训练是一种损失采样" class="headerlink" title="3.4 patchwise 训练是一种损失采样"></a>3.4 patchwise 训练是一种损失采样</h3><blockquote><p>Sampling in patchwise training can correct class imbalance  and mitigate the spatial correlation of dense patches . In fully convolutional training, class balance can also be achieved by weighting the loss, and loss sampling can be used to address spatial correlation. </p></blockquote><p>patchwise可以实现纠正类的不平衡性和缓和像素密集图的空间相关性。</p><p>前者全卷积网络可以通过改变loss的权重实现，后者通过patchwise实现。</p><h2 id="4-分割框架"><a href="#4-分割框架" class="headerlink" title="4 分割框架"></a>4 分割框架</h2><h3 id="4-1-从分类到-dense-FCN"><a href="#4-1-从分类到-dense-FCN" class="headerlink" title="4.1 从分类到 dense FCN"></a>4.1 从分类到 dense FCN</h3><p>使用VGG16效果比较好，用固定的学习率效果更好，通过额外的数据可以提高mIU。</p><h3 id="4-2-结合-“是什么”-和“在哪里”"><a href="#4-2-结合-“是什么”-和“在哪里”" class="headerlink" title="4.2 结合 “是什么” 和“在哪里”"></a>4.2 结合 “是什么” 和“在哪里”</h3><p><img src="E:\GitHub1\blog\source\image\FCN读书笔记\图2.JPG" alt=""></p><h3 id="4-3-实验"><a href="#4-3-实验" class="headerlink" title="4.3 实验"></a>4.3 实验</h3><h4 id="优化参数"><a href="#优化参数" class="headerlink" title="优化参数"></a>优化参数</h4><ul><li><p>使用SGD，momentum =0.9 </p><p>FCN-AlexNet, FCN-VGG16, and FCN-GoogLeNet  学习率分别为10^(-3), 10^(-4), 和 5^(-5) </p></li><li><p>L2正则化 weight= 5^(-4) 或者 2^(-4)</p></li></ul><h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><p>全局微调效果要好于局部微调</p><h3 id="分类平衡"><a href="#分类平衡" class="headerlink" title="分类平衡"></a>分类平衡</h3><p>全卷积训练通过权重对损失函数进行类平衡</p><h2 id="测量方法"><a href="#测量方法" class="headerlink" title="测量方法"></a>测量方法</h2><p><strong>度量</strong>　我们从常见的语义分割和场景解析评估中提出四种度量，它们在像素准确率和在联合的区域交叉上是不同的。令 n_ij 为类别 i 的被预测为类别 j 的像素数量，有 n_ij 个不同的类别，令</p><p><img src="E:\GitHub1\blog\source\image\FCN读书笔记\图4.png" alt=""></p><p>为类别 i 的像素总的数量。我们将计算：</p><p><img src="E:\GitHub1\blog\source\image\FCN读书笔记\图5.png" alt=""></p><h2 id="附录-A-IU-上界"><a href="#附录-A-IU-上界" class="headerlink" title="附录 A　IU 上界"></a>附录 A　IU 上界</h2><blockquote><p>我们通过下采样 ground truth 图像，然后再次对它们进行上采样，来模拟可以获得最好的结果，其伴随着特定的下采样因子。</p></blockquote><table><thead><tr><th style="text-align:center">factor</th><th style="text-align:center">mIU</th></tr></thead><tbody><tr><td style="text-align:center">128</td><td style="text-align:center">50.9</td></tr><tr><td style="text-align:center">64</td><td style="text-align:center">73.3</td></tr><tr><td style="text-align:center">32</td><td style="text-align:center">86.1</td></tr><tr><td style="text-align:center">16</td><td style="text-align:center">92.8</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">96.4</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">98.5</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经典语义分割网络FCN，读书笔记，文献翻译&lt;/p&gt;
&lt;p&gt;参考译文：&lt;a href=&quot;https://www.jianshu.com/p/91c5db272725&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;用于语义分割的全卷积网络FCN（UC Berkeley&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
      <category term="文献" scheme="https://Cxiada.github.io/tags/%E6%96%87%E7%8C%AE/"/>
    
  </entry>
  
  <entry>
    <title>SegNet读书笔记</title>
    <link href="https://Cxiada.github.io/2018/12/14/SegNet%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>https://Cxiada.github.io/2018/12/14/SegNet读书笔记/</id>
    <published>2018-12-14T04:00:00.000Z</published>
    <updated>2018-12-17T14:28:19.197Z</updated>
    
    <content type="html"><![CDATA[<p>经典语义分割网络SegNet，读书笔记，文献翻译</p><a id="more"></a>  <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>We present a novel and practical deep fully convolutional neural network architecture for <strong><u>semantic pixel-wise segmentation</u></strong> termed SegNet.</p><p>逐个像素语义分割</p><p> This core trainable segmentation engine consists of an <strong><u>encoder network</u></strong>, a corresponding <strong><u>decoder network</u></strong> followed by <strong><u>a pixel-wise classification layer</u></strong>. </p><p>SegNet<strong><u>核心是编码网络</u></strong>+<strong><u>像素级分类层</u></strong>+<u><strong>解码网络</strong></u></p><p>The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network . The role of the decoder network is to map the <strong><u>low resolution encoder feature maps</u></strong> to full input resolution feature maps for pixel-wise classification. </p><p>编码网络取了VGG16的前13层（附带BN层）。解码网络就是将<strong><u>低分辨率的特征图</u></strong>解码为输入图片的分辨率图。</p><p>The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses <strong><u>pooling indices</u></strong> computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce <strong><u>dense feature maps</u></strong>. </p><p>SegNet网络核心是对低分辨的特征图进行上采样解码。在编码时使用了<strong><u>池化索引</u></strong>来帮助解码时的非线性上采样。这是上采样的特征图是稀疏的，用可训练的滤波器来产生<strong><u>密集的特征图</u></strong>。</p></blockquote><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><blockquote><p>In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images. Now there is an active interest for <strong>semantic pixel-wise labelling</strong>. However, some of these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, <strong>appear coarse</strong>. This is primarily because <strong>max pooling and sub-sampling reduce feature map resolution</strong>. Our motivation to design SegNet arises from this need to map low resolution features to input resolution for pixel-wise classification. This mapping must produce features which are useful for <strong><u>accurate boundary localization</u></strong> </p><p>用深度学习的方法逐像素地分类，目前的算法得到的结果还比较粗糙，主要是最大池化层和下采样造成特征图分辨率的下降。SegNet核心是为了从低分辨率特征图到输入分辨率的映射，这种映射必须产生<strong><u>精确的边界定位</u></strong>。</p><p>Of these, the appropriate decoders use the max-pooling indices received from the corresponding encoder to perform non-linear upsampling of their input feature maps.Reusing max-pooling indices in the decoding process has several practical advantages; (i) it improves boundary delineation , (ii) it reduces the number of parameters enabling end-to-end training, and (iii) this form of upsampling can be incorporated into any encoder-decoder architecture such as FCN , with only a little modification. </p><p>好处有三个，增强边界划分能力、减少端到端的训练参数、移植能力强</p><p>Most recent deep architectures for segmentation have identical encoder networks, i.e VGG16, but differ in the form of the decoder network, training and inference. Another common feature is they have trainable parameters in the order of hundreds of millions and thus encounter difficulties in performing end-to end training . The difficulty of training these networks has led to multi-stage training , appending networks to a <strong><u>pre-trained architecture</u></strong> such as FCN , use of supporting aids such as <u><strong>region proposals for inference</strong></u>, <strong><u>disjoint training of classification and segmentation networks</u></strong> and use of additional training data for pre-training  or for full training. In addition, performance boosting post-processing techniques have also been popular. Although all these factors improve performance on challenging benchmarks , it is unfortunately difficult from their quantitative results to disentangle the key design factors necessary to achieve good performance. We therefore analysed the decoding process used in some of these approaches and reveal their pros and cons </p><p>有一些网络训练参数达到百万级别，通常训练的需要额外的辅助。<strong><u>多阶段训练</u></strong>、<strong><u>区域建议</u></strong>、<strong><u>分类和分割网络不相交训练</u></strong>。</p><p>We evaluate the performance of SegNet on two scene segmentation tasks, CamVid road scene segmentation and SUN RGB-D indoor scene segmentation </p><p>使用CamVid 道路场景分割和SUN RGB-D室内场景分割作为基准。</p></blockquote><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="E:\GitHub1\blog\source\image\SegNet读书笔记\图1.png" alt="架构"></p><p><img src="E:\GitHub1\blog\source\image\SegNet读书笔记\图2.PNG" alt="反卷积"></p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><blockquote><p>In order to perform a controlled benchmark we used the  <strong>SGD solver</strong>  with a fixed <strong>learning rate of 10-3</strong> and <strong><u>momentum of 0.9</u></strong>. The optimization was performed for more than 100 epochs through the dataset until no further performance increase was observed. Dropout of 0.5 was added to the end of deeper convolutional layers in all models to prevent overfitting. For the road scenes which have 11 classes we used a mini-batch size of 5 and for indoor scenes with 37 classeswe used a mini-batch size of 4. </p></blockquote><p>输入尺寸都为360*480。使用SGD，固定的学习率1e-3和<strong><u>0.9的动量</u></strong>，epoch 超过100，直到不再变化。对于11分类采用batchsize=5、37分类batchsize=4。</p><blockquote><p>我们测试了SegNet、FCN、DeepLab-LargFOV、DeconvNet。</p></blockquote><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><blockquote><p>To compare the quantitative performance of the different decoder variants, we use three commonly used performance measures: <strong><u>global accuracy (G)</u></strong> which measures the percentage of pixels correctly classified in the dataset, <strong><u>class average accuracy (C)</u></strong> is the mean of the predictive accuracy over all classes and <strong><u>mean intersection over union (mIoU)</u></strong> over all classes as used in the Pascal VOC12 challenge. The mIoU metric is a more stringent metric than class average accuracy since it penalizes false positive predictions. However, mIoU metric is not optimized for directly through the class balanced cross-entropy loss. </p></blockquote><p>分析指标主要是<strong><u>全局精度</u></strong>、<strong><u>平均分类精度</u></strong>、<strong><u>平均交并比</u></strong>。平均交并比是一个更严格的指标，它不是通过交叉熵直接优化的。</p><blockquote><p>note that The mIoU metric  does not always correspond to human qualitative judgements (ranks) of good quality segmentation. They show with examples that mIoU favours region smoothness and does not evaluate boundary accuracy </p></blockquote><p>平均交并比更倾向于平滑的边界。</p><blockquote><p>The key idea in computing a semantic contour score is to evaluate the F1-measure  which involves computing the precision and recall values between the predicted and ground truth class boundary given a pixel tolerance distance. We used a value of 0.75% of the image diagonal as the tolerance distance. The F1- measure for each class that is present in the ground truth test image is averaged to produce an image F1-measure. Then we compute the whole test set average, denoted the <strong>boundary F1-measure</strong> (BF) by average the image F1 measures. </p></blockquote><p><strong>boundary F1-measure (BF)</strong> 指标可以衡量边界轮廓。</p><blockquote><p>This shows that using a larger decoder is not enough but it is also important to capture encoder<br>feature map information to learn better, particular the fine grained contour information  </p></blockquote><p>增加解码网络有助于结果精度的提高，尤其是边缘提取。</p><h2 id="道路场景分割"><a href="#道路场景分割" class="headerlink" title="道路场景分割"></a>道路场景分割</h2><blockquote><p>输入尺寸都为360*480。使用SGD，固定的学习率1e-3和0.9的动量，batchsize=5</p></blockquote><p>增大训练样本可以提高平均分类精度和平均交并比。</p><p>在CamVid数据集中，训练迭代次数在40K、80K和超过80K，对于最后一次，训练直到精度不再提高或者出现过拟合现象。</p><blockquote><p>Here we note also that over-fitting was not an issue in training these larger models。</p><p><strong>过拟合现象不需要我们着重要考虑的。</strong></p></blockquote><table><thead><tr><th>SegNet</th><th>G</th><th>mIOU</th><th>BF</th></tr></thead><tbody><tr><td>3.5K dataset-traning -140K</td><td>90.40</td><td>60.10</td><td>46.84</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经典语义分割网络SegNet，读书笔记，文献翻译&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
      <category term="文献" scheme="https://Cxiada.github.io/tags/%E6%96%87%E7%8C%AE/"/>
    
  </entry>
  
  <entry>
    <title>PYtorch之torch.nn</title>
    <link href="https://Cxiada.github.io/2018/12/13/PYtorch%E4%B9%8Btorch.nn%E5%AD%A6%E4%B9%A0/"/>
    <id>https://Cxiada.github.io/2018/12/13/PYtorch之torch.nn学习/</id>
    <published>2018-12-13T04:00:00.000Z</published>
    <updated>2018-12-13T12:04:48.950Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch基础，主要解释torch.nn中的函数方法|卷积网络学习</p><a id="more"></a>  <p>[TOC]</p><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p><code>add_module(*name*, *module*)</code>添加子模块</p><p><code>apply(*fn*)</code>为所有子模块添加函数</p><p><code>children()</code>返回当前子模块的迭代器</p><p><code>named_children()</code></p><p><code>modules()</code>返回所有子模块</p><p><code>named_modules()</code></p><p><code>parameters(*recurse=True*)</code>返回参数迭代器</p><p><code>named_parameters(*prefix=&#39;&#39;*, *recurse=True*)</code>prefix用来加前缀</p><p><code>cpu()/cuda(*device=None*)</code></p><p><code>eval()/train()</code></p><p><code>double()/float()/half</code></p><p><code>load_state_dict(*state_dict*, *strict=True*)</code></p><p><code>state_dict(*destination=None*, *prefix=&#39;&#39;*, *keep_vars=False*)</code></p><h4 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h4><blockquote><p>所有类都要继承的基类</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">       x = F.relu(self.conv1(x))</span><br><span class="line">       <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure><h4 id="add-module"><a href="#add-module" class="headerlink" title="add_module"></a>add_module</h4><blockquote><p>添加子模块</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_module(name, module)</span><br></pre></td></tr></table></figure><h4 id="apply-fn"><a href="#apply-fn" class="headerlink" title="apply(fn)"></a>apply(<em>fn</em>)</h4><blockquote><p>将函数引用到所有子模块</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">        print(m)</span><br><span class="line">        <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">            m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">            print(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.apply(init_weights)</span><br></pre></td></tr></table></figure><h4 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for idx, m in enumerate(net.modules()):  #模块迭代器</span><br><span class="line">&gt;&gt;&gt; for param in model.parameters():         #参数迭代器</span><br><span class="line">&gt;&gt;&gt; for child in model.children():           #子模块迭代器</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span><br><span class="line">&gt;&gt;&gt; for name, param in self.named_parameters():</span><br><span class="line">&gt;&gt;&gt; for name, module in model.named_children():   </span><br><span class="line">&gt;&gt;&gt;     if name in [&apos;conv4&apos;, &apos;conv5&apos;]:    #返回的name的一个用法</span><br><span class="line">&gt;&gt;&gt;         print(module)</span><br></pre></td></tr></table></figure><h4 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h4><table><thead><tr><th>Parameters</th><th>data (<em>Tensor</em>）</th></tr></thead><tbody><tr><td></td><td><strong>requires_grad</strong> (<strong><em>bool</em></strong>, <strong><em>optional</em></strong>)</td></tr></tbody></table><h4 id="type-dst-type"><a href="#type-dst-type" class="headerlink" title="type(dst_type)"></a>type(<em>dst_type</em>)</h4><h3 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h3><blockquote><p>一个时序容器</p><p>Modules按顺序传入</p><p>或者传入OrderDict</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu1'</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu2'</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure><h3 id="ModuleList"><a href="#ModuleList" class="headerlink" title="ModuleList"></a>ModuleList</h3><blockquote><p>相当于创建一个list</p><p>可以使用<code>append</code>(<em>module</em>)和<code>extend</code>(<em>modules</em>)和<code>insert</code>(<em>index</em>, <em>module</em>)修改已有的list</p><p><code>append</code>(<em>module</em>)    将module加到队尾</p><p><code>extend</code>(<em>modules</em>)  将modules组加到队尾</p><p><code>insert</code>(<em>index</em>, <em>module</em>)  插入module</p></blockquote><h3 id="ModuleDict"><a href="#ModuleDict" class="headerlink" title="ModuleDict"></a>ModuleDict</h3><blockquote><p>相当于创建一个Dict    功能同ModuleList</p><p><code>clear</code>()</p><p><code>items</code>()</p><p><code>keys</code>()</p><p><code>values</code>()</p><p><code>pop</code>(<em>key</em>)</p><p><code>update</code>(<em>parameters</em>)</p></blockquote><p>还有类似的<strong>ParameterList</strong>和<strong>ParameterDict</strong></p><h3 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h3><h4 id="2D卷积层-Conv2d"><a href="#2D卷积层-Conv2d" class="headerlink" title="2D卷积层|Conv2d"></a>2D卷积层|Conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h5 id="输出维度公式"><a href="#输出维度公式" class="headerlink" title="输出维度公式"></a>输出维度公式</h5><blockquote><p>Shape:</p></blockquote><ul><li>Input<br>$$<br>(N, C_{in}, H_{in}, W_{in})<br>$$</li></ul><ul><li>Output<br>$$<br>(N,C_{out},H_{out},W_{out})<br>$$</li></ul><p>$$<br>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding} -\text{kernel_size} }{\text{stride}} + 1\right\rfloor<br>$$</p><h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>数据的最后一列可能会因为kernal大小设定不当而被丢弃（大部分发生在kernal大小不能被输入整除时，适当的padding可以避免这个问题）</p><h4 id="2D反卷积层-ConvTranspose2d"><a href="#2D反卷积层-ConvTranspose2d" class="headerlink" title="2D反卷积层|ConvTranspose2d"></a>2D反卷积层|ConvTranspose2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, output_padding=<span class="number">0</span>, groups=<span class="number">1</span>, bias=<span class="keyword">True</span>, dilation=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h5 id="输出维度公式-1"><a href="#输出维度公式-1" class="headerlink" title="输出维度公式"></a>输出维度公式</h5><p>$$<br>H_{out} = (H_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{kernel_size} + \text{output_padding}<br>$$</p><h4 id="2D最大池化层-MaxPool2d"><a href="#2D最大池化层-MaxPool2d" class="headerlink" title="2D最大池化层|MaxPool2d"></a>2D最大池化层|MaxPool2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool2d(kernel_size, stride=<span class="keyword">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="keyword">False</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">return_indices  <span class="keyword">True</span>时返回最大值的序号，用于上采样</span><br><span class="line">ceil_mode   默认向下取整模式</span><br><span class="line"></span><br><span class="line"><span class="comment">#常用kernel_size=stride，same核</span></span><br><span class="line">MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>)  <span class="comment">#缩小一倍</span></span><br></pre></td></tr></table></figure><h5 id="输出维度公式-2"><a href="#输出维度公式-2" class="headerlink" title="输出维度公式"></a>输出维度公式</h5><p>$$<br>H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding} - \text{kernel_size} }{\text{stride}} + 1\right\rfloor<br>$$</p><h4 id="2D反池化层-MaxUnpool2d"><a href="#2D反池化层-MaxUnpool2d" class="headerlink" title="2D反池化层|MaxUnpool2d"></a>2D反池化层|MaxUnpool2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxUnpool2d(kernel_size, stride=<span class="keyword">None</span>, padding=<span class="number">0</span>, output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数参照池化层选取</span></span><br><span class="line">MaxUnpool2d(x4pool, id4, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, output_size=size4)</span><br></pre></td></tr></table></figure><p>input</p><ul><li>input: 输入Tensor</li><li>indices: 从<code>MaxPool2d</code>得到的最大值序号</li><li>output_size (optional): 想要的输出维度</li></ul><h4 id="2D平均池化层-AvgPool2d"><a href="#2D平均池化层-AvgPool2d" class="headerlink" title="2D平均池化层|AvgPool2d"></a>2D平均池化层|AvgPool2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.AvgPool2d(kernel_size, stride=<span class="keyword">None</span>, padding=<span class="number">0</span>, ceil_mode=<span class="keyword">False</span>, count_include_pad=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h4 id="2D标准化层-BatchNorm2d"><a href="#2D标准化层-BatchNorm2d" class="headerlink" title="2D标准化层|BatchNorm2d"></a>2D标准化层|BatchNorm2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">num_features=C <span class="comment">#通道数</span></span><br><span class="line">affine=<span class="keyword">True</span> <span class="comment">#添加可学习的仿射变换参数</span></span><br><span class="line">track_running_stats=<span class="keyword">True</span>  <span class="comment">#用于验证预测</span></span><br></pre></td></tr></table></figure><p>$$<br>y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta<br>$$</p><h5 id="与InstanceNorm2d的比较"><a href="#与InstanceNorm2d的比较" class="headerlink" title="与InstanceNorm2d的比较"></a>与InstanceNorm2d的比较</h5><p><strong>BN作用于一个Batch                               IN作用于单张图片</strong></p><p>因此，在特征提取分类等任务中，需要不同图片的联动，选BN</p><p>在生成式类任务如风格迁移等，每张照片有自己的style，选IN</p><h4 id="超分辨卷积-PixelShuffle"><a href="#超分辨卷积-PixelShuffle" class="headerlink" title="超分辨卷积|PixelShuffle"></a>超分辨卷积|PixelShuffle</h4><p>sub-pixel convolution</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.PixelShuffle(upscale_factor)</span><br></pre></td></tr></table></figure><p>将低分辨率图片变为高分辨率图片<br>$$<br> (*, C \times r^2, H, W)—-&gt; (C, H \times r, W \times r)<br>$$</p><h4 id="2D双线性上采样-UpsamplingBilinear2d"><a href="#2D双线性上采样-UpsamplingBilinear2d" class="headerlink" title="2D双线性上采样|UpsamplingBilinear2d"></a>2D双线性上采样|UpsamplingBilinear2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.UpsamplingBilinear2d(size=<span class="keyword">None</span>, scale_factor=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">size=(H_out, W_out)</span><br></pre></td></tr></table></figure><h3 id="损失函数-Loss-functions"><a href="#损失函数-Loss-functions" class="headerlink" title="损失函数|Loss functions"></a>损失函数|Loss functions</h3><p>L1Loss</p><p>MSELoss</p><p>CrossEntropyLoss</p><p>NLLLoss|NLLLoss2d</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch基础，主要解释torch.nn中的函数方法|卷积网络学习&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Pytorch" scheme="https://Cxiada.github.io/categories/python/Pytorch/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
      <category term="CNN" scheme="https://Cxiada.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>VGG读书笔记</title>
    <link href="https://Cxiada.github.io/2018/12/11/VGG%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>https://Cxiada.github.io/2018/12/11/VGG读书笔记/</id>
    <published>2018-12-11T04:00:00.000Z</published>
    <updated>2018-12-13T12:09:01.669Z</updated>
    
    <content type="html"><![CDATA[<p>经典网络VGG，读书笔记，文献翻译</p><a id="more"></a>  <p>[TOC]</p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><blockquote><p>本文主要研究卷积神经网络深度对准确率的影响。利用<code>3*3</code>的小卷积核加大网络的深度。结果表明加深网络深度至<strong>16-19层</strong>可以极大地提高准确率。同时模型对其他数据集也有很好的泛化能力。</p></blockquote><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><blockquote><p>ILSVRC 比赛大大促进了deep visual recognition architectures 的发展。</p><p>In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers. </p></blockquote><p>本文主要解决了网路的深度问题。<strong>固定其他超参数</strong>，通过增加更多的<code>3*3</code>的小卷积核加深网络深度。</p><h3 id="2-卷积神经网络配置"><a href="#2-卷积神经网络配置" class="headerlink" title="2 卷积神经网络配置"></a>2 卷积神经网络配置</h3><p>大量使用<code>3*3</code>的小卷积核。应为它是捕捉左/右/上/下、中心的最小核。</p><h3 id="2-1-架构（ARCHITECTURE-）"><a href="#2-1-架构（ARCHITECTURE-）" class="headerlink" title="2.1 架构（ARCHITECTURE ）"></a>2.1 架构（ARCHITECTURE ）</h3><blockquote><p>输入为224*224的RGB图片。</p><p>预处理为减去每一个像素RGB的均值。</p><p>后面是一系列的卷积层，使用3*3的小卷积核。</p><p>后面是全连接层，两个4096通道，后面一个1000通道分类器。</p></blockquote><h3 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h3><blockquote><p>下图列出了A-E，5中网络深度随之增大。A有8个卷积网络和3个全连接层；E有16个卷积网络和3个全连接层。<strong>通道数从64开始，512结束。</strong></p></blockquote><p><img src="https://blog-hexo-1257634887.cos.ap-beijing.myqcloud.com/VGG%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/VGG%E6%A8%A1%E5%9E%8B.jpg" alt="VGG网络"></p><h3 id="2-3-讨论"><a href="#2-3-讨论" class="headerlink" title="2.3 讨论"></a>2.3 讨论</h3><blockquote><p>VGG使用小卷积核的原因是：两个<code>3*3</code>的卷积层和一个<code>5*5</code>的卷积层具有相同的感受野；三个<code>3*3</code>的卷积层和一个<code>7*7</code>的卷积层具有相同的感受野。</p></blockquote><h3 id="3-1-训练"><a href="#3-1-训练" class="headerlink" title="3.1 训练"></a>3.1 训练</h3><h4 id="训练超参数"><a href="#训练超参数" class="headerlink" title="训练超参数"></a>训练超参数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batchsize=<span class="number">256</span>,</span><br><span class="line">使用MBGD(mini-batch gradient descent)梯度下降,lr=<span class="number">0.01</span>  每当精度不再下降时 lr=lr*<span class="number">0.1</span></span><br><span class="line">monentum=<span class="number">0.9</span> Drop(<span class="number">0.5</span>)</span><br><span class="line">weight decay  L2=<span class="number">5e-4</span></span><br><span class="line"></span><br><span class="line">最终结果lr--&gt;<span class="number">1e-5</span>,iterations=<span class="number">370</span>K(epochs=<span class="number">74</span>)</span><br></pre></td></tr></table></figure><h4 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a>数据集处理</h4><blockquote><p>预处理包括 ：</p><p>裁剪尺寸=224*224，图片随机水平翻转和随机RBG颜色偏移</p></blockquote><h5 id="训练尺寸"><a href="#训练尺寸" class="headerlink" title="训练尺寸"></a>训练尺寸</h5><p><strong>图片尺寸应&gt;=224</strong></p><blockquote><p>我们考虑了两种设置训练尺寸S的方法：</p><p>第一种固定尺寸。我们设置S=256和S=384。为了加速S=384的训练，我们使用小学习率lr=1e-3</p><p>第二种设置多尺度训练，每一幅图片随机裁剪S–&gt;[Smin,Smax]，(256,512)。可以看成是缩放抖动。</p></blockquote><h3 id="3-2-测试"><a href="#3-2-测试" class="headerlink" title="3.2 测试"></a>3.2 测试</h3><blockquote><p>将全连接层换成卷积层。第一个全连接层转换为<code>7*7</code>的卷积；第二第三个转换为<code>1*1</code>的卷积。这样是为了适应输出不同尺寸的图片的测试结果。最后累加得到<code>1*1*num_classes</code>的结果。</p></blockquote><h3 id="4-分类实验"><a href="#4-分类实验" class="headerlink" title="4 分类实验"></a>4 分类实验</h3><blockquote><p>数据集使用ILSVRC-2012。数据集包含1000个类。训练图片1.3M，交叉验证50K，测试图片100K。有两种测试指标top-1和top-5。</p></blockquote><p><strong>top5</strong>是指给出5个分类结果，有一个对即可。</p><h4 id="4-1-单尺寸评估"><a href="#4-1-单尺寸评估" class="headerlink" title="4.1 单尺寸评估"></a>4.1 单尺寸评估</h4><blockquote><p>单尺寸（S = 256 or S = 384 ）</p><p>首先，LRN网络对提高精度没有帮助。</p><p>其次，提高网络深度有助于提高精度，同时使用<code>3*3</code>卷积核比使用<code>1*1</code>的卷积核要有效果。使用5*5的卷积核效果也没有更好。</p><p>最后，尺寸抖动技术有更好的效果。</p></blockquote><p><strong>深层网络需要使用小卷积核</strong></p><p><strong>使用尺寸抖动技术better</strong></p><h4 id="4-2多尺寸评估"><a href="#4-2多尺寸评估" class="headerlink" title="4.2多尺寸评估"></a>4.2多尺寸评估</h4><h4 id="4-3-Multi-crop-评估"><a href="#4-3-Multi-crop-评估" class="headerlink" title="4.3 Multi-crop 评估"></a>4.3 Multi-crop 评估</h4><blockquote><p>Multi-crop evaluation：对图像进行随机裁剪，然后通过网络预测每一个样本的类，最后对所有结果求平均</p><p>Dense evaluation：利用FCN思想，改全连接层为卷积层，将原图送入网络预测结果。得到score map，最后求平均。</p></blockquote><p>Multi-crop相当于padding补充0。<strong><em>better</em></strong></p><p>Dense evaluation相当于padding补充相邻像素点，并增大了感受野。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经典网络VGG，读书笔记，文献翻译&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
      <category term="文献" scheme="https://Cxiada.github.io/tags/%E6%96%87%E7%8C%AE/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch模型保存与载入</title>
    <link href="https://Cxiada.github.io/2018/11/28/Pytorch%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%BD%BD%E5%85%A5/"/>
    <id>https://Cxiada.github.io/2018/11/28/Pytorch模型保存与载入/</id>
    <published>2018-11-28T04:00:02.000Z</published>
    <updated>2018-12-13T08:50:29.047Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch基础，主要是保存模型数据，载入模型数据</p><a id="more"></a>  <p>[TOC]</p><h4 id="pytorch保存数据"><a href="#pytorch保存数据" class="headerlink" title="pytorch保存数据"></a>pytorch保存数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">state=&#123;<span class="string">'net'</span>:model.state_dict(),<span class="string">'optimizer'</span>:optimizer.state_dict(),<span class="string">'epoch'</span>:Epoch&#125;</span><br><span class="line">dir=<span class="string">'model.pth'</span></span><br><span class="line">torch.save(state,dir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line"><span class="comment"># torch.save(cnn1,'cnn1.pkl')  #保存整个网络结构与参数</span></span><br><span class="line"><span class="comment"># torch.save(cnn1.state_dict(),'cnn1_params.pkl') #只保存参数</span></span><br></pre></td></tr></table></figure><h4 id="pytorch载入"><a href="#pytorch载入" class="headerlink" title="pytorch载入"></a>pytorch载入</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dir=<span class="string">'model.pth'</span></span><br><span class="line">state=torch.load(dir)</span><br><span class="line">model.load_state_dict(state[<span class="string">'net'</span>])</span><br><span class="line">optimizer.load_state_dict(state[<span class="string">'optimizer'</span>])</span><br><span class="line">Epoch=state[<span class="string">'epoch'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">net=Net()</span><br><span class="line">net.load_state_dict(torch.load(<span class="string">'net.pth'</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pytorch基础，主要是保存模型数据，载入模型数据&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Pytorch" scheme="https://Cxiada.github.io/categories/python/Pytorch/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch加载数据集</title>
    <link href="https://Cxiada.github.io/2018/11/28/PyTorch%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>https://Cxiada.github.io/2018/11/28/PyTorch加载数据集/</id>
    <published>2018-11-28T04:00:01.000Z</published>
    <updated>2018-12-13T08:49:09.437Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch基础，主要是数据的加载和保存</p><a id="more"></a>  <p>[TOC]</p><h4 id="Dataset类"><a href="#Dataset类" class="headerlink" title="Dataset类"></a>Dataset类</h4><blockquote><p>在PyTorch中，加载数据集，需要继承torch.utils.data.dataset 中的Dataset类重写方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; from torch.utils.data.dataset import Dataset</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">dataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ...)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, ...)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> (...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self, ...)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(...)</span><br></pre></td></tr></table></figure><blockquote><p>一个读取图像数据集的例子</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DatasetFromjpg</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            root (string): 文件路径</span></span><br><span class="line"><span class="string">            transform: transform 操作</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.root=root</span><br><span class="line">        imgs=os.listdir(root)</span><br><span class="line">        <span class="comment">#这里只是加载路径</span></span><br><span class="line">        self.imgs=[os.path.join(root,img) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]  <span class="comment">#list[图片路径]</span></span><br><span class="line">        self.transforms = transforms</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment">#cat.10.jpg/dog.20.jpg      #dog-&gt;1  cat-&gt;0</span></span><br><span class="line">        img_path=self.imgs[index]</span><br><span class="line">        label=<span class="number">1</span> <span class="keyword">if</span> <span class="string">'dog'</span> <span class="keyword">in</span> img_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        pil=Image.open(img_path)</span><br><span class="line">        <span class="keyword">if</span> self.transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img_as_tensor = self.transforms(pil)</span><br><span class="line">            <span class="keyword">return</span> (img_as_tensor, label)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img_as_np = np.asarray(pil)</span><br><span class="line">            <span class="keyword">return</span> (img_as_np,label)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure><h4 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h4><blockquote><p>针对PIL图片常用函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; from torchvision import transforms</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#有次序！！！</span></span><br><span class="line">transformations = /</span><br><span class="line">transforms.Compose([transforms.Resize(<span class="number">224</span>),     <span class="comment">#调整大小，保持比例  最小边-224</span></span><br><span class="line">                    transforms.CenterCrop(<span class="number">234</span>), <span class="comment">#中心裁剪</span></span><br><span class="line">                    transforms.RandomCrop, <span class="comment">#</span></span><br><span class="line">                    transforms.RandomSizedCrop, <span class="comment">#</span></span><br><span class="line">                    transforms.Pad              <span class="comment">#填充</span></span><br><span class="line">                    transforms.ToTensor(),</span><br><span class="line">                    transforms.ToPILImage(),</span><br><span class="line">                    transforms.Normalize(mean=[<span class="number">.5</span>,<span class="number">.5</span>,<span class="number">.5</span>],std=[<span class="number">.5</span>,<span class="number">.5</span>,<span class="number">.5</span>])</span><br><span class="line">                                      ])  <span class="comment"># 将读取的图片变化</span></span><br></pre></td></tr></table></figure><h4 id="DataLoader类"><a href="#DataLoader类" class="headerlink" title="DataLoader类"></a>DataLoader类</h4><blockquote><p>加载数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; from torch.utils.data import DataLoader</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_loader=DataLoader(dataset=dataset,</span><br><span class="line">                        batch_size=<span class="number">10</span>,</span><br><span class="line">                        shuffle=<span class="keyword">True</span>,  <span class="comment">#乱序</span></span><br><span class="line">                        num_workers=<span class="number">2</span>, <span class="comment">#CPU多线程</span></span><br><span class="line">                        sample  <span class="comment">#样本抽样</span></span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line"><span class="comment">#DataLoader是一个迭代对象</span></span><br><span class="line"><span class="keyword">from</span>  torchvision.utils <span class="keyword">import</span> make_grid,save_image</span><br><span class="line">dataiter = iter(train_loader)</span><br><span class="line">images,labels=next(dataiter)</span><br><span class="line">photos=make_grid(imgs,<span class="number">3</span>)  <span class="comment">#tensor类型 3*3网格</span></span><br><span class="line"><span class="comment"># plt.imshow(photos)</span></span><br><span class="line">save_image(photos,<span class="string">'save_photos.jpg'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch基础，主要是数据的加载和保存&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Pytorch" scheme="https://Cxiada.github.io/categories/python/Pytorch/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习</title>
    <link href="https://Cxiada.github.io/2018/11/28/PyTorch%E5%AD%A6%E4%B9%A0/"/>
    <id>https://Cxiada.github.io/2018/11/28/PyTorch学习/</id>
    <published>2018-11-28T04:00:00.000Z</published>
    <updated>2018-12-13T08:50:07.078Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习基础，主要是简单的创建数组、算数操作、类型转换等</p><a id="more"></a>  <p>[TOC]</p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#inplace方式</span></span><br><span class="line">a.add(b)   </span><br><span class="line">a.add_(b)  <span class="comment">#会改变自身数据    函数名以_结尾的都是inplace方式</span></span><br></pre></td></tr></table></figure><h4 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h4><h5 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建Tensor</span></span><br><span class="line">tensor([list])       类似np.array的构造函数 </span><br><span class="line">ones(*sizes)        全<span class="number">1</span>Tensor </span><br><span class="line">zeros(*sizes)       全<span class="number">0</span>Tensor </span><br><span class="line">eye(*sizes)         对角线为<span class="number">1</span>，其他为<span class="number">0</span> </span><br><span class="line">arange(s,e,step)     从s到e，步长为step </span><br><span class="line">linspace(s,e,steps) 从s到e，均匀切分成steps份 </span><br><span class="line">rand/randn(*sizes)  均匀/标准分布 </span><br><span class="line">normal(mean,std)    正态分布/均匀分布 </span><br><span class="line">randperm(m)         随机排列 </span><br><span class="line"></span><br><span class="line"><span class="comment">#a.size()  是tuple的子类</span></span><br></pre></td></tr></table></figure><h5 id="Tensorlist-numpy"><a href="#Tensorlist-numpy" class="headerlink" title="Tensorlist/numpy"></a>Tensor<--->list/numpy</---></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Tensor&lt;---&gt;list</span></span><br><span class="line">a.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Tensor&lt;-----&gt;numpy</span></span><br><span class="line">torch.from_numpy</span><br><span class="line">a.numpy()</span><br></pre></td></tr></table></figure><h5 id="算术操作"><a href="#算术操作" class="headerlink" title="算术操作"></a>算术操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">torch.view     <span class="comment">#=reshape</span></span><br><span class="line">torch.unsqueeze(d)  <span class="comment">#增加维度   从0开始</span></span><br><span class="line">torch.squeeze(d)    <span class="comment">#压缩为1的维度</span></span><br><span class="line">torch.mm       <span class="comment">#矩阵乘法</span></span><br><span class="line">t <span class="comment">#转置 </span></span><br><span class="line">clamp(input, min, max) <span class="comment">#超过min和max部分截断 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#逐元素操作</span></span><br><span class="line">abs/sqrt/div/exp/fmod/log/pow.. <span class="comment">#绝对值/平方根/除法/指数/求余/求幂.. </span></span><br><span class="line">cos/sin/asin/atan2/cosh <span class="comment">#相关三角函数 </span></span><br><span class="line">ceil/round/floor/trunc <span class="comment">#上取整/四舍五入/下取整/只保留整数部分 </span></span><br><span class="line">sigmod/tanh <span class="comment">#激活函数 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#归并操作</span></span><br><span class="line">mean/sum/median/mode <span class="comment">#均值/和/中位数/众数 </span></span><br><span class="line">norm/dist <span class="comment">#范数/距离 </span></span><br><span class="line">std/var <span class="comment">#标准差/方差 </span></span><br><span class="line">cumsum/cumprod <span class="comment">#累加/累乘 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#线性代数</span></span><br><span class="line">trace <span class="comment">#对角线元素之和(矩阵的迹) </span></span><br><span class="line">diag <span class="comment">#对角线元素 </span></span><br><span class="line">triu/tril <span class="comment">#矩阵的上三角/下三角，可指定偏移量 </span></span><br><span class="line">mm/bmm <span class="comment">#矩阵乘法，batch的矩阵乘法 </span></span><br><span class="line">addmm/addbmm/addmv/addr/badbmm <span class="comment">#矩阵运算 </span></span><br><span class="line">dot/cross <span class="comment">#内积/外积 </span></span><br><span class="line">inverse <span class="comment">#求逆矩阵 </span></span><br><span class="line">svd <span class="comment">#奇异值分解</span></span><br></pre></td></tr></table></figure><h4 id="Tensor类型转换"><a href="#Tensor类型转换" class="headerlink" title="Tensor类型转换"></a>Tensor类型转换</h4><table><thead><tr><th>Data type</th><th>dtype</th><th>Tensor types</th></tr></thead><tbody><tr><td>32-bit floating point</td><td><code>torch.float32</code> or <code>torch.float</code></td><td><code>torch.*.FloatTensor</code></td></tr><tr><td>64-bit floating point</td><td><code>torch.float64</code> or <code>torch.double</code></td><td><code>torch.*.DoubleTensor</code></td></tr><tr><td>16-bit floating point</td><td><code>torch.float16</code> or <code>torch.half</code></td><td><code>torch.*.HalfTensor</code></td></tr><tr><td>8-bit integer (unsigned)</td><td><code>torch.uint8</code></td><td><code>torch.*.ByteTensor</code></td></tr><tr><td>8-bit integer (signed)</td><td><code>torch.int8</code></td><td><code>torch.*.CharTensor</code></td></tr><tr><td>16-bit integer (signed)</td><td><code>torch.int16</code> or <code>torch.short</code></td><td><code>torch.*.ShortTensor</code></td></tr><tr><td>32-bit integer (signed)</td><td><code>torch.int32</code> or <code>torch.int</code></td><td><code>torch.*.IntTensor</code></td></tr><tr><td>64-bit integer (signed)</td><td><code>torch.int64</code> or <code>torch.long</code></td><td><code>torch.*.LongTensor</code></td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">b=a.type(torch.FloatTensor)</span><br><span class="line">tensor.to(torch.float64)</span><br><span class="line"></span><br><span class="line"><span class="comment">#CPU---&gt;GPU</span></span><br><span class="line">tensor.cuda()</span><br><span class="line">tensor.cpu()</span><br></pre></td></tr></table></figure><h4 id="保存与载入"><a href="#保存与载入" class="headerlink" title="保存与载入"></a>保存与载入</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(a,<span class="string">'a.pth'</span>)</span><br><span class="line">torch.loas(a,<span class="string">'a.pth'</span>)</span><br></pre></td></tr></table></figure><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p><img src="E:\GitHub1\blog\source\image\PyTorch学习\autograd_Variable.png" alt="变量结构"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data  tensor类型</span><br><span class="line">grad  梯度</span><br><span class="line">creator</span><br></pre></td></tr></table></figure><h4 id="模型相关"><a href="#模型相关" class="headerlink" title="模型相关"></a>模型相关</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#针对dropout</span></span><br><span class="line">model.training()  <span class="comment">#表明在train模式</span></span><br><span class="line">model.eval()      <span class="comment">#针对测试</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.save(net.state_dict(),<span class="string">'net.pth'</span>)</span><br><span class="line">net=Net()</span><br><span class="line">net.load_state_dict(torch.load(<span class="string">'net.pth'</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;机器学习基础，主要是简单的创建数组、算数操作、类型转换等&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Pytorch" scheme="https://Cxiada.github.io/categories/python/Pytorch/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Sklearn回顾</title>
    <link href="https://Cxiada.github.io/2018/11/15/Sklearn%E5%9B%9E%E9%A1%BE/"/>
    <id>https://Cxiada.github.io/2018/11/15/Sklearn回顾/</id>
    <published>2018-11-15T04:00:00.000Z</published>
    <updated>2018-12-13T08:51:16.161Z</updated>
    
    <content type="html"><![CDATA[<p>简单回顾了Sklearn的基本操作</p><a id="more"></a>  <h3 id="Sklearn回顾"><a href="#Sklearn回顾" class="headerlink" title="Sklearn回顾"></a>Sklearn回顾</h3><h4 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,Y_train,Y_test=train_test_split(X,y,</span><br><span class="line">                                               train_size=<span class="number">0.7</span>,</span><br><span class="line">                                               random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h4><p>！图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> cross_val_score</span><br><span class="line">cross_val_score(model,X,y,cv=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><p>！图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">mat=confusion_matrix(Y_test,y_pred)</span><br><span class="line">sns.heatmap(mat,square=<span class="keyword">True</span>,annot=<span class="keyword">True</span>,cbar=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><h4 id="简单线性回归"><a href="#简单线性回归" class="headerlink" title="简单线性回归"></a>简单线性回归</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,</span><br><span class="line">                                               train_size=<span class="number">0.7</span>,</span><br><span class="line">                                               random_state=<span class="number">1</span>)</span><br><span class="line">X_train=X_train.values</span><br><span class="line">y_train=y_train.values</span><br><span class="line"><span class="comment">#2.导入模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">model=LinearRegression(fit_intercept=<span class="keyword">True</span>)  <span class="comment">#设置截距</span></span><br><span class="line"><span class="comment">#3.模型适配（训练）</span></span><br><span class="line">model.fit(X_train,y_train)</span><br><span class="line"><span class="comment">#4.可以查看参数矩阵</span></span><br><span class="line">model.coef_,model.intercept_</span><br><span class="line"><span class="comment">#5.预测</span></span><br><span class="line">y_pred=model.predict(X_test)</span><br><span class="line"><span class="comment">#6.评估</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test,y_pred)</span><br><span class="line"><span class="comment">#(7.混淆矩阵)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">mat=confusion_matrix(y_test,y_pred)</span><br><span class="line">sns.heatmap(mat,square=<span class="keyword">True</span>,annot=<span class="keyword">True</span>,cbar=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>简单高斯贝叶斯</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line">model=GaussianNB()</span><br><span class="line">model.fit(X_train,y_train)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单回顾了Sklearn的基本操作&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Sklearn" scheme="https://Cxiada.github.io/categories/python/Sklearn/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib回顾</title>
    <link href="https://Cxiada.github.io/2018/11/13/Matplotlib%E5%9B%9E%E9%A1%BE/"/>
    <id>https://Cxiada.github.io/2018/11/13/Matplotlib回顾/</id>
    <published>2018-11-13T04:00:00.000Z</published>
    <updated>2018-12-13T08:51:26.466Z</updated>
    
    <content type="html"><![CDATA[<p>简单回顾了Matplotlib的基本操作</p><a id="more"></a>  <h3 id="Matplotlib回顾"><a href="#Matplotlib回顾" class="headerlink" title="Matplotlib回顾"></a>Matplotlib回顾</h3><h4 id="开始使用"><a href="#开始使用" class="headerlink" title="开始使用"></a>开始使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#%matploylib notebook</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot.show()   <span class="comment">#一个session只能使用一次，通常在末行开启</span></span><br><span class="line"></span><br><span class="line">fig.savefig(<span class="string">'my_photo.png'</span>)  <span class="comment">#保存图片</span></span><br></pre></td></tr></table></figure><h4 id="简易线性图—-gt-颜色-坐标轴-标签"><a href="#简易线性图—-gt-颜色-坐标轴-标签" class="headerlink" title="简易线性图—&gt;颜色/坐标轴/标签"></a>简易线性图—&gt;颜色/坐标轴/标签</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">fig=plot.figure()</span><br><span class="line">ax=plt.axes()</span><br><span class="line"></span><br><span class="line"><span class="comment">#直线</span></span><br><span class="line">x=np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1000</span>)</span><br><span class="line">plt.plot(x,np.sin(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#颜色</span></span><br><span class="line">pltpltpltplt.plot(x,np.sin(x<span class="number">-0</span>)，color=<span class="string">'blue'</span>)</span><br><span class="line">pltpltplt.plot(x,np.sin(x<span class="number">-1</span>),color=<span class="string">'g'</span>)     <span class="comment">#rgbcmyk</span></span><br><span class="line">pltplt.plot(x,np.sin(x<span class="number">-2</span>),color=<span class="string">'0.75'</span>)  <span class="comment">#灰度：0-1</span></span><br><span class="line">plt.plot(x,np.sin(x<span class="number">-3</span>),color=<span class="string">'#FFFFDD'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#元素</span></span><br><span class="line">plt.plot(x,np.sin(x<span class="number">-0</span>)，linestyle=<span class="string">'-'</span>)</span><br><span class="line"><span class="comment">#  '-'   '--'    '-.'     ':'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#颜色+元素   ‘--g’</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#坐标轴</span></span><br><span class="line">plt.xlim(<span class="number">-1</span>,<span class="number">1</span>) <span class="comment">#正序</span></span><br><span class="line">plt.ylim(<span class="number">10</span>,<span class="number">1</span>) <span class="comment">#逆序</span></span><br><span class="line">plt.axis(<span class="string">'tight'</span>)  <span class="comment">#自动收缩空白区域</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#标签</span></span><br><span class="line">title</span><br><span class="line">xlabel  ylabel</span><br><span class="line">legend</span><br></pre></td></tr></table></figure><h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x,y, <span class="string">'o'</span>  ,color=<span class="string">'r'</span>)  </span><br><span class="line"><span class="comment">#'o'   '.'  ','  'x'   '+'   'v'  '^'  '&lt;'  '&gt;'  's'  'd'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#一步到位</span></span><br><span class="line">ptl.plot(x,y,<span class="string">'-ok'</span>)  <span class="comment">#直线(-) 圆圈(o)  黑色(k)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置参数</span></span><br><span class="line">plt.plot(x,y,<span class="string">'-ok'</span>,</span><br><span class="line">        markersize=<span class="number">15</span>,  <span class="comment">#点大小</span></span><br><span class="line">        linewidth=<span class="number">4</span>,    <span class="comment">#线宽</span></span><br><span class="line">        markerfacecolor=<span class="string">'white'</span>,  <span class="comment">#中心点颜色</span></span><br><span class="line">        markeredgewidth=<span class="number">2</span>         <span class="comment">#点边缘</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">plt.scatter(x,y,marker=<span class="string">'o'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#不同颜色深浅的散点图</span></span><br><span class="line">rng=np.random.RandomState(<span class="number">0</span>)</span><br><span class="line">x=rng.randn(<span class="number">100</span>)</span><br><span class="line">y=rng.randn(<span class="number">100</span>)</span><br><span class="line">colors=rng.rand(<span class="number">100</span>)</span><br><span class="line">sizes=rng.rand(<span class="number">100</span>)*<span class="number">1000</span></span><br><span class="line">plt.scatter(x,y,c=colors,s=sizes,alpha=<span class="number">0.4</span>,cmap=<span class="string">'viridis'</span>)</span><br><span class="line">plt.colorbar()  <span class="comment">#颜色栏</span></span><br></pre></td></tr></table></figure><h4 id="三维图像"><a href="#三维图像" class="headerlink" title="三维图像"></a>三维图像</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x=np.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">50</span>)</span><br><span class="line">y=np.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">40</span>)</span><br><span class="line">X,Y=np.meshgrid(x,y)</span><br><span class="line">Z=np.sin(X)**<span class="number">10</span>+np.cos(<span class="number">10</span>+X*Y)+np.cos(Y)</span><br><span class="line">plt.contour(X,Y,Z,</span><br><span class="line">            <span class="number">20</span>,    <span class="comment">#将数据范围等分20</span></span><br><span class="line">            cmap=<span class="string">'RdGy'</span></span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">plt.contourf()</span><br><span class="line"></span><br><span class="line"><span class="comment">#!!!!!!!!!!!!左上角(0,0)</span></span><br><span class="line">plt.imshow(Z,</span><br><span class="line">          extent=[<span class="number">0</span>,<span class="number">5</span>,<span class="number">0</span>,<span class="number">5</span>],  <span class="comment">#坐标轴</span></span><br><span class="line">          origin=<span class="string">'lower'</span>,     <span class="comment">#</span></span><br><span class="line">          cmap=<span class="string">'RdGy'</span>)</span><br></pre></td></tr></table></figure><h4 id="统计特性—-gt-直方图-联合分布-seaborn"><a href="#统计特性—-gt-直方图-联合分布-seaborn" class="headerlink" title="统计特性—-&gt;直方图/联合分布/seaborn"></a>统计特性—-&gt;直方图/联合分布/seaborn</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一维</span></span><br><span class="line">plt.hist(data,</span><br><span class="line">        bins=<span class="number">30</span>,</span><br><span class="line">        normed=<span class="keyword">True</span>,</span><br><span class="line">        alpha=<span class="number">0.5</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">counts,bin_edges=np.histogram(data,bins=<span class="number">30</span>)  <span class="comment">#只输出结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#二维</span></span><br><span class="line">plt.hist2d</span><br><span class="line">plt.hexbin  <span class="comment">#正六边形</span></span><br><span class="line">np.histhistogram2d</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns  <span class="comment">#使用基于matpltlib的高级API</span></span><br><span class="line">sns.hist</span><br><span class="line">sns.kdeplot(x,shade=<span class="keyword">True</span>)  <span class="comment">#直方图平滑</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#hist和KDE相结合</span></span><br><span class="line">sns.distplot</span><br><span class="line"></span><br><span class="line"><span class="comment">#二维联合分布</span></span><br><span class="line">sns.jointplot(x,y,kind=<span class="string">'kde'</span>)  <span class="comment">#二维kde</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#sns.jointplot('x','y',data=DataFrame,kind='hex')</span></span><br><span class="line">sns.jointplot(x,y,kind=<span class="string">'hex'</span>)  </span><br><span class="line">sns.jointplot(x,y,kind=<span class="string">'reg'</span>)  <span class="comment">#带回归的联合分布 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#矩阵图  多变量两两关系图</span></span><br><span class="line">sns.pairplot(DataFrame,hue=<span class="string">'species'</span>,size=<span class="number">2.5</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单回顾了Matplotlib的基本操作&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Matplotlib" scheme="https://Cxiada.github.io/categories/python/Matplotlib/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Pandas回顾</title>
    <link href="https://Cxiada.github.io/2018/11/13/Pandas%E5%9B%9E%E9%A1%BE/"/>
    <id>https://Cxiada.github.io/2018/11/13/Pandas回顾/</id>
    <published>2018-11-13T04:00:00.000Z</published>
    <updated>2018-12-13T08:51:52.952Z</updated>
    
    <content type="html"><![CDATA[<p>简单回顾了Pandas的基本操作</p><a id="more"></a>  <h3 id="Pandas回顾"><a href="#Pandas回顾" class="headerlink" title="Pandas回顾"></a>Pandas回顾</h3><p>[TOC]</p><h4 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#字典-数组</span></span><br><span class="line">Series=pd.Series([<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>])   <span class="comment">#直接式</span></span><br><span class="line"><span class="comment">#Series=pd.Series(&#123;0:11,1:12,2:13,3:14&#125;)  #字典式</span></span><br><span class="line">index=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">Series=pd.Series(data,index=index)  <span class="comment">#索引</span></span><br><span class="line"></span><br><span class="line">Series</span><br><span class="line"><span class="comment">#index values</span></span><br><span class="line"><span class="comment">#0    11</span></span><br><span class="line"><span class="comment">#1    12</span></span><br><span class="line"><span class="comment">#2    13</span></span><br><span class="line"><span class="comment">#3    14</span></span><br><span class="line"><span class="comment">#dtype: int64</span></span><br><span class="line"><span class="comment">#有index  values   dtype </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据索引</span></span><br><span class="line">Series.loc   显式从<span class="number">1</span>开始</span><br><span class="line">Series.iloc  隐式 从<span class="number">0</span>开始，左闭右开</span><br></pre></td></tr></table></figure><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#字典-Series</span></span><br><span class="line">data=pd.DataFrame(&#123;<span class="string">'col1'</span>:Series1,</span><br><span class="line">                   <span class="string">'col2'</span>:Series2 &#125;)</span><br><span class="line"></span><br><span class="line">data</span><br><span class="line"><span class="comment">#index   col1  col2</span></span><br><span class="line"><span class="comment">#Califor  11    12</span></span><br><span class="line"><span class="comment">#Flori    12    12</span></span><br><span class="line"><span class="comment">#Texas    2     3</span></span><br><span class="line"><span class="comment">#有index columns</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#索引方式</span></span><br><span class="line">data[<span class="string">'col1'</span>]</span><br><span class="line">data.loc[:,<span class="string">'col1'</span>:<span class="string">'col2'</span>]</span><br><span class="line"><span class="comment">#data.ix[:3,'col1':'col2']</span></span><br></pre></td></tr></table></figure><h4 id="索引对齐"><a href="#索引对齐" class="headerlink" title="索引对齐"></a>索引对齐</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">A=pd.DataFrame</span><br><span class="line"><span class="comment">#   a   b</span></span><br><span class="line"><span class="comment">#0  2   3</span></span><br><span class="line"><span class="comment">#1  4   5</span></span><br><span class="line">B=pd.DataFrame</span><br><span class="line"><span class="comment">#   a   b   c</span></span><br><span class="line"><span class="comment">#0  1   1   1</span></span><br><span class="line"><span class="comment">#1  1   1   1</span></span><br><span class="line"><span class="comment">#2  1   1   1</span></span><br><span class="line"></span><br><span class="line">A.add(B,fill.value=A.stack().mean())</span><br><span class="line"><span class="comment">#0  3   4   NaN</span></span><br><span class="line"><span class="comment">#1  5   6   NaN</span></span><br><span class="line"><span class="comment">#2  NaN NaN NaN   #NaN=fill.value</span></span><br><span class="line"></span><br><span class="line">+   add</span><br><span class="line">-   sub</span><br><span class="line">*   mul</span><br><span class="line">/   div</span><br><span class="line">//  floordiv</span><br><span class="line">%   mod</span><br><span class="line">**  pow</span><br></pre></td></tr></table></figure><h4 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">isnull     <span class="comment">#返回mark</span></span><br><span class="line">notnull    <span class="comment">#与isnull相反</span></span><br><span class="line"></span><br><span class="line">dropna(axis，how,thresh)      <span class="comment">#返回剔除缺失值的数据</span></span><br><span class="line">axis=<span class="number">1</span>       <span class="comment">#按维度剔除</span></span><br><span class="line">how=all/any  <span class="comment">#剔除要求</span></span><br><span class="line">thresh=<span class="number">3</span>     <span class="comment">#最小数目</span></span><br><span class="line"></span><br><span class="line">fillna(method,axis)     <span class="comment">#返回填充缺失值的数据副本</span></span><br><span class="line">method=ffill/bfill  <span class="comment">#继承前/后数值来填充</span></span><br><span class="line">axis                <span class="comment">#继承方向</span></span><br></pre></td></tr></table></figure><h4 id="高维数据多级索引"><a href="#高维数据多级索引" class="headerlink" title="高维数据多级索引"></a>高维数据多级索引</h4><h5 id="二维Series"><a href="#二维Series" class="headerlink" title="二维Series"></a>二维Series</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#二维Series</span></span><br><span class="line">data=np.random.randn(<span class="number">4</span>)</span><br><span class="line">index=[(<span class="string">'a'</span>,<span class="number">1</span>),(<span class="string">'a'</span>,<span class="number">2</span>),(<span class="string">'b'</span>,<span class="number">1</span>),(<span class="string">'b'</span>,<span class="number">2</span>)]  <span class="comment">#元组</span></span><br><span class="line">Series=pd.Series(data,index=index)</span><br><span class="line"></span><br><span class="line">index=pd.MultiIndex.from_tuples(index)  <span class="comment">#从元组转换</span></span><br><span class="line"><span class="comment"># MultiIndex(levels=[['a', 'b'], [1, 2]],</span></span><br><span class="line"><span class="comment">#           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])</span></span><br><span class="line">Series=Series.reindex(index)   <span class="comment">#多级索引</span></span><br><span class="line">data=Seires.unstack()   <span class="comment">#与DataFrame转换   ---stack</span></span><br></pre></td></tr></table></figure><h5 id="高维DataFrame"><a href="#高维DataFrame" class="headerlink" title="高维DataFrame"></a>高维DataFrame</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#高维DataFrame</span></span><br><span class="line">oridata=np.random.randn(<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">index=pd.MultiIndex.from_product([[<span class="string">'a'</span>,<span class="string">'b'</span>],[<span class="number">1</span>,<span class="number">2</span>]],</span><br><span class="line">                                names=[<span class="string">'level1'</span>,<span class="string">'level2'</span>])</span><br><span class="line">columns=pd.MultiIndex.from_product([[<span class="string">'data1'</span>,<span class="string">'data2'</span>,<span class="string">'data3'</span>],[<span class="string">'temp1'</span>,<span class="string">'temp2'</span>]],</span><br><span class="line">                                  names=[<span class="string">'label1'</span>,<span class="string">'label2'</span>])</span><br><span class="line">data=pd.DataFrame(data=oridata,index=index,columns=columns)  <span class="comment">#两个索引的笛卡尔积</span></span><br><span class="line"></span><br><span class="line">data</span><br><span class="line"><span class="comment">#              label1      data1          data2          data3</span></span><br><span class="line"><span class="comment">#              label2  temp1  temp2   temp1  temp2    temp1  temp2</span></span><br><span class="line"><span class="comment">#level1  level2</span></span><br><span class="line"><span class="comment">#a        1</span></span><br><span class="line"><span class="comment">#         2</span></span><br><span class="line"><span class="comment">#b        1</span></span><br><span class="line"><span class="comment">#         2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据索引</span></span><br><span class="line">idx=pd.IndexSlice</span><br><span class="line">data.loc[ idx[:,<span class="number">1</span>]  ,idx[:,<span class="string">'temp2'</span>] ]</span><br></pre></td></tr></table></figure><h5 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对DataFrame的操作</span><br><span class="line">sort.index()   <span class="comment">#方法--排序(字典序)</span></span><br><span class="line"></span><br><span class="line">对Series的操作</span><br><span class="line">unstack(level=<span class="number">0</span>)  <span class="comment">#---&gt;二维 </span></span><br><span class="line">stack             <span class="comment">#反操作</span></span><br><span class="line"></span><br><span class="line">reset_index(name=<span class="string">'level1'</span>)  <span class="comment">#行列转换</span></span><br></pre></td></tr></table></figure><h4 id="数据集操作"><a href="#数据集操作" class="headerlink" title="数据集操作"></a>数据集操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#简单合并</span></span><br><span class="line">pd.concat(   </span><br><span class="line">         ([DataFrame1,DataFrame2]),</span><br><span class="line">         axis=<span class="number">0</span>    <span class="comment">#合并方向</span></span><br><span class="line">         verity_integrity=<span class="keyword">True</span>   <span class="comment">#重复索引触发异常</span></span><br><span class="line">         ignore_index=<span class="keyword">True</span>       <span class="comment">#创造新的索引</span></span><br><span class="line">         key=[<span class="string">'DataFrame1'</span>,<span class="string">'DataFrame2'</span>]  <span class="comment">#增加多级索引</span></span><br><span class="line">         join=<span class="string">'inner'</span>/<span class="string">'outer'</span>  <span class="comment">#列名的交集/合集</span></span><br><span class="line">         )</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据库式合并</span></span><br><span class="line">pd.merge([DataFrame1,DataFrame2])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单回顾了Pandas的基本操作&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Pandas" scheme="https://Cxiada.github.io/categories/python/Pandas/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>Numpy回顾</title>
    <link href="https://Cxiada.github.io/2018/11/12/Numpy%E5%9B%9E%E9%A1%BE/"/>
    <id>https://Cxiada.github.io/2018/11/12/Numpy回顾/</id>
    <published>2018-11-12T04:00:00.000Z</published>
    <updated>2018-12-13T08:51:40.619Z</updated>
    
    <content type="html"><![CDATA[<p>简单回顾了Numpy的基本操作</p><a id="more"></a>  <h3 id="Numpy回顾"><a href="#Numpy回顾" class="headerlink" title="Numpy回顾"></a>Numpy回顾</h3><p>[TOC]</p><h4 id="创建数组（zeros-arange-random-eye"><a href="#创建数组（zeros-arange-random-eye" class="headerlink" title="创建数组（zeros/arange/random/eye)"></a>创建数组（zeros/arange/random/eye)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.zeros( (<span class="number">2</span>,<span class="number">3</span>),dtype=int )  <span class="comment">#ones</span></span><br><span class="line"></span><br><span class="line">np.arange(<span class="number">0</span>,<span class="number">20</span>,<span class="number">2</span>) <span class="comment">#步长为2</span></span><br><span class="line"></span><br><span class="line">np.random.randn(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#random normal randint</span></span><br><span class="line"></span><br><span class="line">np.eye(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h4 id="数组性质（ndim-shape-dtype"><a href="#数组性质（ndim-shape-dtype" class="headerlink" title="数组性质（ndim/shape/dtype)"></a>数组性质（ndim/shape/dtype)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">x=np.random.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">print(<span class="string">'x.ndim:  '</span>,x.ndim)</span><br><span class="line">print(<span class="string">'x.shape:  '</span>,x.shape)</span><br><span class="line">print(<span class="string">'x.dtype:  '</span>,x.dtype)</span><br></pre></td></tr></table></figure><h4 id="副本与副本copy"><a href="#副本与副本copy" class="headerlink" title="副本与副本copy"></a>副本与副本copy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=np.random.randn(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">x1=x[:<span class="number">2</span>,:<span class="number">2</span>]</span><br><span class="line">x1[:,:]=<span class="number">666</span>;  <span class="comment">#非副本,会改变原数组的值</span></span><br><span class="line"></span><br><span class="line">x2=x[:<span class="number">2</span>,:<span class="number">2</span>].copy()  <span class="comment">#副本</span></span><br></pre></td></tr></table></figure><h4 id="数组拼接-concatenate-vstack-hstack"><a href="#数组拼接-concatenate-vstack-hstack" class="headerlink" title="数组拼接(concatenate/vstack/hstack)"></a>数组拼接(concatenate/vstack/hstack)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=np.arange(<span class="number">4</span>)</span><br><span class="line">y=np.arange(<span class="number">3</span>)</span><br><span class="line">z=np.concatenate([x,y]) </span><br><span class="line"></span><br><span class="line"><span class="comment">#np.vstack   np.hstack</span></span><br><span class="line"><span class="comment">#np.concatenate([x,y],axis=0) 高维拼接</span></span><br></pre></td></tr></table></figure><h4 id="通用算术"><a href="#通用算术" class="headerlink" title="通用算术"></a>通用算术</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//  <span class="comment">#取整除数</span></span><br><span class="line">**</span><br><span class="line">%</span><br><span class="line">abs</span><br><span class="line">sin   <span class="comment">#弧度</span></span><br><span class="line">expm1 <span class="comment">#针对小数值</span></span><br><span class="line">log1p <span class="comment">#针对小数值</span></span><br></pre></td></tr></table></figure><h4 id="scipy-special"><a href="#scipy-special" class="headerlink" title="scipy.special"></a>scipy.special</h4><h4 id="统计特性"><a href="#统计特性" class="headerlink" title="统计特性"></a>统计特性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sum   <span class="comment">#对整个数组的聚合</span></span><br><span class="line">sum(axis=<span class="number">0</span>)  <span class="comment">#对单一维度进行折叠---相当于对合并行</span></span><br><span class="line"></span><br><span class="line">prod <span class="comment">#积</span></span><br><span class="line">mean <span class="comment">#均值</span></span><br><span class="line">std  <span class="comment">#标准差</span></span><br><span class="line">var  <span class="comment">#方差</span></span><br><span class="line">argmin  <span class="comment">#最小值索引  argmax</span></span><br><span class="line">median  <span class="comment">#中位数</span></span><br><span class="line">percentile  <span class="comment">#统计值  percentile(a,25)--分位数</span></span><br><span class="line">any  <span class="comment">#只要有一个为True</span></span><br><span class="line">all</span><br></pre></td></tr></table></figure><h4 id="布尔运算符"><a href="#布尔运算符" class="headerlink" title="布尔运算符"></a>布尔运算符</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#逐位</span></span><br><span class="line">&amp;</span><br><span class="line">|</span><br><span class="line">^  <span class="comment">#xor</span></span><br><span class="line">~</span><br></pre></td></tr></table></figure><h4 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h4><p><img src="https://blog-hexo-1257634887.cos.ap-beijing.myqcloud.com/Numpy%E5%9B%9E%E9%A1%BE/%E5%B9%BF%E6%92%AD.jpg" alt="广播jpg"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#规则1：两个数组维度不一样，那么小的数组左边补1</span></span><br><span class="line"><span class="comment">#规则2：两个数组维度不匹配，则扩展数值=1的维度</span></span><br><span class="line"><span class="comment">#否则引发错误</span></span><br></pre></td></tr></table></figure></p><h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下标传递</span></span><br><span class="line">x=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">ind=[<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">x[ind]=[<span class="number">1</span>,<span class="number">-1</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">x=[   [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">      [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">      [<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]]</span><br><span class="line">row=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">col=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line">x[row,col].shape=(<span class="number">3</span>,)  <span class="comment">#第一个索引为下标1，第二个索引为下标2</span></span><br><span class="line"><span class="comment">#本质是先广播，后取值</span></span><br></pre></td></tr></table></figure><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sort     <span class="comment">#=sort  默认从小到大</span></span><br><span class="line">sort(x,axis=<span class="number">0</span>)  <span class="comment">#会丢失数据之间的依赖关系</span></span><br><span class="line"></span><br><span class="line">argsort  <span class="comment">#=sorted</span></span><br><span class="line"></span><br><span class="line">partition  <span class="comment">#部分排序</span></span><br><span class="line">partition(x,<span class="number">3</span>,axis=<span class="number">0</span>)  <span class="comment">#前三个位置为最小的三个值</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单回顾了Numpy的基本操作&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="https://Cxiada.github.io/categories/python/"/>
    
      <category term="Numpy" scheme="https://Cxiada.github.io/categories/python/Numpy/"/>
    
    
      <category term="机器学习基础" scheme="https://Cxiada.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>C++中cout输出格式</title>
    <link href="https://Cxiada.github.io/2018/09/18/C++%E4%B8%ADcout%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F/"/>
    <id>https://Cxiada.github.io/2018/09/18/C++中cout输出格式/</id>
    <published>2018-09-18T15:37:14.000Z</published>
    <updated>2018-09-18T16:04:22.291Z</updated>
    
    <content type="html"><![CDATA[<p>C++中cout的日常使用<br><a id="more"></a></p><h2 id="控制输出格式"><a href="#控制输出格式" class="headerlink" title="控制输出格式"></a>控制输出格式</h2><p><img src="https://blog-hexo-1257634887.cos.ap-beijing.myqcloud.com/C%2B%2B%E4%B8%ADcout%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F/20180918113953583.png" alt="cout输出格式控制"></p><p>##控制格式输出<br><img src="https://blog-hexo-1257634887.cos.ap-beijing.myqcloud.com/C%2B%2B%E4%B8%ADcout%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F/20180918114535596.png" alt="cout格式输出控制"></p><hr><h2 id="使用样例"><a href="#使用样例" class="headerlink" title="使用样例"></a>使用样例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt; //使用流对象成员函数</span><br><span class="line">#include &lt;iomanip&gt;//使用控制符设置</span><br><span class="line">using namespace std;</span><br><span class="line">int main()&#123;</span><br><span class="line">    int a = 31;</span><br><span class="line">    cout.width(8);  //使用流对象成员函数</span><br><span class="line">    cout.fill(&apos;*&apos;);</span><br><span class="line">    cout.unsetf(ios::dec);</span><br><span class="line">    cout.setf(ios::hex);</span><br><span class="line">    cout &lt;&lt; a &lt;&lt; endl;</span><br><span class="line">    cout &lt;&lt; setw(8) &lt;&lt; setfill(&apos;*&apos;) &lt;&lt; resetiosflags(ios::dec) &lt;&lt; setiosflags(ios::hex) &lt;&lt; a &lt;&lt; endl;  //使用控制符设置</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><p>参考自<a href="https://blog.csdn.net/u012707739/article/details/77824635" target="_blank" rel="noopener">乌托的博客</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;C++中cout的日常使用&lt;br&gt;
    
    </summary>
    
      <category term="C/C++" scheme="https://Cxiada.github.io/categories/C-C/"/>
    
    
      <category term="C/C++" scheme="https://Cxiada.github.io/tags/C-C/"/>
    
      <category term="cout" scheme="https://Cxiada.github.io/tags/cout/"/>
    
  </entry>
  
  <entry>
    <title>C++复习一：语言基础</title>
    <link href="https://Cxiada.github.io/2018/09/18/C++%E5%A4%8D%E4%B9%A0%E4%B8%80%EF%BC%9A/"/>
    <id>https://Cxiada.github.io/2018/09/18/C++复习一：/</id>
    <published>2018-09-18T15:06:09.000Z</published>
    <updated>2018-09-19T15:38:15.123Z</updated>
    
    <content type="html"><![CDATA[<ul><li>算术类型</li><li>变量初始化</li><li>const|constexpr</li><li>auto|decltype</li><li>类型转换<a id="more"></a>  </li></ul><h2 id="算术类型"><a href="#算术类型" class="headerlink" title="算术类型"></a>算术类型</h2><table><thead><tr><th>布尔类型</th><th>bool</th></tr></thead><tbody><tr><td>字符类型</td><td>char</td></tr><tr><td>整数类型</td><td>int  / long</td></tr><tr><td>浮点类型</td><td>float /  double</td></tr></tbody></table><p>用<font color="#EE0000"> <strong>sizeof</strong></font>求解占用的内存字节数<br>带符号 signed<br>不带符号 unsigned</p><h2 id="变量初始化"><a href="#变量初始化" class="headerlink" title="变量初始化"></a>变量初始化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int count=0;  //拷贝初始化</span><br><span class="line">int count(0); //直接初始化</span><br><span class="line">int count=&#123;0&#125;; //列表初始化</span><br></pre></td></tr></table></figure><p><strong>左值和右值</strong>  </p><hr><h2 id="const-constexpr与auto-decltype"><a href="#const-constexpr与auto-decltype" class="headerlink" title="const|constexpr与auto|decltype"></a>const|constexpr与auto|decltype</h2><p>const 常量<br>constexpr 常量表达式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">const int size=20；//运行时常量</span><br><span class="line">constexpr int limits=size+20;//编译技术时会是一个常数</span><br></pre></td></tr></table></figure></p><p>auto 自动类型说明符<br>decltype 类型指示符<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auto x=1.0;  //编译时用正确的类型替换auto</span><br><span class="line">decltype(ci) x=1; //只定义x为ci的类型，不赋初值</span><br><span class="line">decltype(f() ) y=sum; //定义y为f()的返回值类型</span><br></pre></td></tr></table></figure></p><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><h3 id="static-cast"><a href="#static-cast" class="headerlink" title="static_cast"></a>static_cast</h3><p>用于将一种数据类型转换成另一种数据类型，使用格式如下：<br>变量1 = static_cast&lt;变量1数据类型&gt;(另外一种数据类型变量或表达式);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int a = 1;</span><br><span class="line">float b;</span><br><span class="line">b = static_cast&lt;float&gt;(a);  //类似于C语言中的b = (float)a;</span><br></pre></td></tr></table></figure></p><h3 id="const-cast"><a href="#const-cast" class="headerlink" title="const_cast"></a>const_cast</h3><p>去除表达式中的const限定。用于指针、引用、变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">const int i=0;</span><br><span class="line">int *j=&amp;i;  //错误</span><br><span class="line">j=const_cast_&lt;int *&gt;(&amp;i); //正确</span><br></pre></td></tr></table></figure></p><h3 id="dynamic-cast"><a href="#dynamic-cast" class="headerlink" title="dynamic_cast"></a>dynamic_cast</h3><p>执行派生类指针或引用与基类指针或引用之间的转换。</p><h3 id="reinterpret-cast-不常用"><a href="#reinterpret-cast-不常用" class="headerlink" title="reinterpret_cast 不常用"></a>reinterpret_cast 不常用</h3><p>从字面意思理解是一个“重新解释的类型转换”。也就是说对任意两个类型之间的变量我们都可以个使用reinterpret_cast在他们之间相互转换，无视类型信息。</p><h2 id="按位运算符"><a href="#按位运算符" class="headerlink" title="按位运算符"></a>按位运算符</h2><p>按位与运算符（&amp;）<br>按位或运算符（|）<br>异或运算符（^）<br>取反运算符（~）<br>左移运算符（&lt;&lt;） 右补0<br>右移运算符（&gt;&gt;） 左补0</p><p><font color="#EE0000"> <strong>不同长度的数据进行位运算</strong></font><br>如果两个不同长度的数据进行位运算时，系统会将二者按右端对齐，然后进行位运算。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;算术类型&lt;/li&gt;
&lt;li&gt;变量初始化&lt;/li&gt;
&lt;li&gt;const|constexpr&lt;/li&gt;
&lt;li&gt;auto|decltype&lt;/li&gt;
&lt;li&gt;类型转换
    
    </summary>
    
      <category term="C/C++" scheme="https://Cxiada.github.io/categories/C-C/"/>
    
    
      <category term="语言基础" scheme="https://Cxiada.github.io/tags/%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/"/>
    
  </entry>
  
  <entry>
    <title>个人博客建站</title>
    <link href="https://Cxiada.github.io/2018/09/16/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99/"/>
    <id>https://Cxiada.github.io/2018/09/16/个人博客建站/</id>
    <published>2018-09-16T05:18:13.000Z</published>
    <updated>2018-09-18T16:21:09.332Z</updated>
    
    <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>  早就想着用Github用建个个人博客，刚开始试了官方推荐的jekyll，后来发现好多人多用hexo建站。我两个都试过了，觉得还是hexo好用<em>关键是hexo官方有中文文档</em>。<br>  目前的配置是Github<em>部署网站</em>+Hexo<em>建站工具</em>+next<em>主题</em>+HexoEditor<em>编辑器</em>+腾讯云<em>图床</em><br><a id="more"></a></p><h2 id="1-下载安装"><a href="#1-下载安装" class="headerlink" title="1.下载安装"></a>1.下载安装</h2><ul><li>下载github安装，其中自带了git。</li><li>在github官网申请注册帐号，需要用到邮箱，注册后一定要记住三个东西：你的用户名(username)，你的密码(password)，以及你的邮箱(email)</li><li>安装<a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.js</a></li><li>新建hexo文件夹，打开 Git Bash。<strong>接下来的命令均在Git Bash中执行</strong></li><li>安装 Hexo : <code>$npm install -g hexo</code></li><li>安装依赖包： <code>$npm install</code></li><li>执行<code>$hexo init</code> 初始化网站<strong>这时候用<code>hexo s</code>就能预览效果了</strong></li><li>新建Github仓库：仓库名<strong>必须</strong>为<code>你的Github名.github.io</code>，要不然就不能使用Github Pages服务了。。。</li></ul><h2 id="2-配置-SSH"><a href="#2-配置-SSH" class="headerlink" title="2.配置 SSH"></a>2.配置 SSH</h2><ul><li>本地生成公钥私钥<br>　<code>$ssh-keygen -t rsa -C &quot;你的邮件地址&quot;</code></li><li>添加公钥到 Github<ul><li>根据上一步的提示，找到公钥文件（默认为id_rsa.pub），用记事本打开，全选并复制。</li><li>登录 Github，右上角 头像 -&gt; Settings —&gt; SSH keys —&gt; Add SSH key。把公钥粘贴到key中，填好title并点击 Add key。</li><li>git bash中输入命令<code>$ssh -T git@github.com</code>，选yes，等待片刻可看到成功提示。</li></ul></li></ul><h2 id="3-NexT主题下载"><a href="#3-NexT主题下载" class="headerlink" title="3. NexT主题下载"></a>3. NexT主题下载</h2><p>　　 NexT 主题是由 <a href="https://github.com/iissnan" target="_blank" rel="noopener">iissnan</a> 大神所制作的一款简洁美观不失逼格的主题。下载方法有以下两种：  </p><ul><li>进入<code>博客根目录/themes/</code>, 执行<code>$git clone https://github.com/iissnan/hexo-theme-next.git</code></li><li>直接进入上面的链接，在项目主页download zip文件，然后解压到<code>博客根目录/themes/</code> 文件夹<h2 id="4-发布"><a href="#4-发布" class="headerlink" title="4. 发布"></a>4. 发布</h2>使用以下两条命令进行发布，发布成功后可在浏览器中使用<code>你的github名.github.io</code>进入你的博客~</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$hexo clean</span><br><span class="line">$hexo d -g</span><br></pre></td></tr></table></figure><h2 id="Hexo-常用命令"><a href="#Hexo-常用命令" class="headerlink" title="Hexo 常用命令"></a>Hexo 常用命令</h2><h3 id="1-本地预览："><a href="#1-本地预览：" class="headerlink" title="1.本地预览："></a>1.本地预览：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$hexo server//或 hexo s</span><br><span class="line">//然后打开浏览器输入localhost:4000可以预览博客效果，用于调试</span><br></pre></td></tr></table></figure><h3 id="2-新建文章"><a href="#2-新建文章" class="headerlink" title="2. 新建文章"></a>2. 新建文章</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$hexo new post &quot;title&quot;</span><br><span class="line">//新文章位置：/source/_posts</span><br></pre></td></tr></table></figure><h3 id="3-新建页面"><a href="#3-新建页面" class="headerlink" title="3. 新建页面"></a>3. 新建页面</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$hexo new page &quot;title&quot;</span><br></pre></td></tr></table></figure><h3 id="4-部署并生成"><a href="#4-部署并生成" class="headerlink" title="4. 部署并生成"></a>4. 部署并生成</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$hexo d -g</span><br></pre></td></tr></table></figure><h3 id="5-清除生成的文件和缓存"><a href="#5-清除生成的文件和缓存" class="headerlink" title="5. 清除生成的文件和缓存"></a>5. 清除生成的文件和缓存</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$hexo clean</span><br></pre></td></tr></table></figure><h3 id="推荐使用HexoEditor编辑器就不用手打命令了。"><a href="#推荐使用HexoEditor编辑器就不用手打命令了。" class="headerlink" title=" 推荐使用HexoEditor编辑器就不用手打命令了。"></a><font color="#EE0000"> <strong>推荐使用HexoEditor编辑器就不用手打命令了。</strong></font></h3><h2 id="4-next主题"><a href="#4-next主题" class="headerlink" title="4. next主题"></a>4. next主题</h2><h2 id="5-HexoEditor"><a href="#5-HexoEditor" class="headerlink" title="5. HexoEditor"></a>5. HexoEditor</h2><p>HexoEditor下载地址：<a href="https://github.com/zhuzhuyule/HexoEditor" target="_blank" rel="noopener">https://github.com/zhuzhuyule/HexoEditor</a><br>里面有详细安装说明<br>Tips<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//If find Error about download Electron faild ,you can try run those</span><br><span class="line">npm install -g electron@1.8.1</span><br></pre></td></tr></table></figure></p><h2 id="6-腾讯云图床"><a href="#6-腾讯云图床" class="headerlink" title="6. 腾讯云图床"></a>6. 腾讯云图床</h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p><a href="https://lovenight.github.io/2015/11/10/Hexo-3-1-1-%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/" target="_blank" rel="noopener">Hexo3.1.1静态博客搭建指南</a></p></li><li><p><a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">NexT使用文档</a></p></li><li><p><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo官方文档（中文版）</a></p></li><li><p><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo官方文档（中文版）</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h1&gt;&lt;p&gt;  早就想着用Github用建个个人博客，刚开始试了官方推荐的jekyll，后来发现好多人多用hexo建站。我两个都试过了，觉得还是hexo好用&lt;em&gt;关键是hexo官方有中文文档&lt;/em&gt;。&lt;br&gt;  目前的配置是Github&lt;em&gt;部署网站&lt;/em&gt;+Hexo&lt;em&gt;建站工具&lt;/em&gt;+next&lt;em&gt;主题&lt;/em&gt;+HexoEditor&lt;em&gt;编辑器&lt;/em&gt;+腾讯云&lt;em&gt;图床&lt;/em&gt;&lt;br&gt;
    
    </summary>
    
      <category term="hexo" scheme="https://Cxiada.github.io/categories/hexo/"/>
    
      <category term="next" scheme="https://Cxiada.github.io/categories/hexo/next/"/>
    
    
      <category term="hexo" scheme="https://Cxiada.github.io/tags/hexo/"/>
    
      <category term="next" scheme="https://Cxiada.github.io/tags/next/"/>
    
  </entry>
  
  <entry>
    <title>创建图床</title>
    <link href="https://Cxiada.github.io/2018/09/15/%E5%88%9B%E5%BB%BA%E5%9B%BE%E5%BA%8A/"/>
    <id>https://Cxiada.github.io/2018/09/15/创建图床/</id>
    <published>2018-09-15T09:22:54.000Z</published>
    <updated>2018-09-18T16:21:16.933Z</updated>
    
    <content type="html"><![CDATA[<ul><li>新浪微博相册图片支持外链，速度快，免费。用新浪微博相册可以为博客、写作平台打造一个免费的图床。  </li><li>也可以使用腾讯云的对象储存，个人实名以后每个月有大量的免费额度，供个人博客使用足够了。  </li></ul><a id="more"></a><h3 id="微博做图床"><a href="#微博做图床" class="headerlink" title=" 微博做图床"></a><font color="#9F79EE"> 微博做图床</font></h3><ol><li>注册微博账号</li><li>上传图片到微博相册</li><li>获取外链：点击图片-查看大图-右键图片复制图片链接，得到如下链接：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://ww1.sinaimg.cn/large/sample.jpg</span><br></pre></td></tr></table></figure><p>同时还有其他大小可以选择<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">http://ww1.sinaimg.cn/thumbnail/sample.jpg   // 缩略图</span><br><span class="line">http://ww1.sinaimg.cn/small/sample.jpg       // 稍微大点的图</span><br><span class="line">http://ww1.sinaimg.cn/bmiddle/sample.jpg     // 再大点的图</span><br><span class="line">http://ww1.sinaimg.cn/large/sample.jpg       // 最大的</span><br></pre></td></tr></table></figure></p><h4 id="推荐使用picgo图库神器上传图片"><a href="#推荐使用picgo图库神器上传图片" class="headerlink" title=" 推荐使用picgo图库神器上传图片"></a><font color="#EE0000"> 推荐使用picgo图库神器上传图片</font></h4><p><img src="https://ws1.sinaimg.cn/small/007jzaWply1fvap7039dyj30jg0jg0u3.jpg" alt="微博图片测试"></p><h3 id="腾讯云oss做图床"><a href="#腾讯云oss做图床" class="headerlink" title=" 腾讯云oss做图床"></a><font color="#9F79EE"> 腾讯云oss做图床</font></h3><ol><li>注册腾讯云账号<br>网上也有推荐七牛云做图床的。但是要上传手持身份证照片。我嫌麻烦。腾讯云只用拿微信做个验证就好了。</li><li><p>创建储存桶。注意有两个关键的配置不能忽略<br>存储桶（bucket）访问权限<br>防盗链设置</p><h5 id="访问权限"><a href="#访问权限" class="headerlink" title="访问权限"></a>访问权限</h5><p>访问权限应设置为公有读私有写。因为我们要去读照片嘛。</p><h5 id="防盗链"><a href="#防盗链" class="headerlink" title="防盗链"></a>防盗链</h5><p>把自己的博客地址加进去  </p></li><li><p>使用腾讯云提供的cos客户端上传图片。右键点击获取图片链接。<br><strong><em>注意：</em></strong>如果没有把本地地址加入白名单，是不能看到预览图的。</p></li></ol><p><img src="https://blog-hexo-1257634887.cos.ap-beijing.myqcloud.com/3.jpg" alt="腾讯云外链测试"></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;新浪微博相册图片支持外链，速度快，免费。用新浪微博相册可以为博客、写作平台打造一个免费的图床。  &lt;/li&gt;
&lt;li&gt;也可以使用腾讯云的对象储存，个人实名以后每个月有大量的免费额度，供个人博客使用足够了。  &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="hexo" scheme="https://Cxiada.github.io/categories/hexo/"/>
    
    
      <category term="hexo" scheme="https://Cxiada.github.io/tags/hexo/"/>
    
  </entry>
  
</feed>
